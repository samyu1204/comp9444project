{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c77fe9",
   "metadata": {},
   "source": [
    "# COMP9444 23T2 Group Project\n",
    "### Leveraging Sensor Fusion for Enhanced Nighttime Object Detection in Autonomous Driving: A Comparative Study of Datasets and Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76861f",
   "metadata": {},
   "source": [
    "# 1. Introduction, Motivation, and/or Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b2962",
   "metadata": {},
   "source": [
    "### Introduction and Motivation\n",
    "Since 2010, Advanced Driver-Assistance Systems (ADAS) and automated driving have rapidly emerged as significant trends in the automotive industry. These developments underline the importance of achieving high-performance on-road object detection under diverse operating conditions, including nighttime and extreme weather scenarios (Waldschmidt, C., Hasch, J. and Menzel, W., 2021). Such environments present unique challenges for traditional vision-based systems, necessitating novel approaches to ensure reliable and robust object detection.\n",
    "![trends](images/motivation/trend.png)\n",
    "\n",
    "Modern day computer vision neural networks often fail to perform well in nighttime object detection (inaccurate detection of objects in low luminosity environments). Nighttime environment factors like shadow, limited luminosity, and visibility makes it challenging for the network to classify objects. With this problem, it can hinder the effectiveness and safety of pre-existing computer vision applications like surveillance, which requires all day monitoring.\n",
    "\n",
    "Solving day/night object detection will definitely bring significant enhancements in the real world, and some key areas of improvements are autonomous driving, surveillance and security systems. This is not only an exciting technical challenge for researchers, but also has the potential to open up new possibilities for neural network computer vision advancements.\n",
    "\n",
    "In recent years, sensor fusion technology has gained traction in addressing these challenges. The integration of various sensors such as camera, radar, and lidar, provides richer and more comprehensive data, which has been shown to enhance object detection capabilities (Nobis, F., Geisslinger, M., Weber, M., Betz, J. and Lienkamp, M., 2019). With the increasing prevalence of commercial vehicles equipped with these multiple sensor systems, sensor fusion methodologies have become an area of active research, particularly in the context of deep learning-based object detection.\n",
    "\n",
    "\n",
    "\n",
    "### Our task\n",
    "  This work aims to conduct a comparative study of state-of-the-art models based on both camera-only and sensor fusion methodologies, to understand the potential benefits of sensor fusion in improving nighttime object detection performance. Moreover, we seek to identify and evaluate the public datasets that can be utilized for future research in this domain. By elucidating the performance differences between various model architectures and data inputs, we hope to provide valuable insights for ongoing advancements in ADAS and automated driving technologies. Furthermore, by assessing the suitability of available datasets for such research, we aim to facilitate further exploration and development in this critical field.\n",
    "\n",
    "### Related works\n",
    "Researchers have made advancements in enhancing accuracy for low-light detection. An example is the REDI low-light enhancement algorithm, which effectively filters noise in low-light conditions and performs detection on the resulting image.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/lowlight.png\" />\n",
    "</div>\n",
    "\n",
    "Here (a) through to (d) are stages of REDI algorithm filtering. However, there are many downsides to this algorithm like loss of details, over-correction, and high computational cost. This would pose a challenge as it would add extra complexity and computational stress on existing models.\n",
    "\n",
    "From our literature review, there are two kinds of approaches for the object detection based on the input sensor: single sensor-based and multiple sensor-based.\n",
    "\n",
    "## Single Sensor-Based Approaches\n",
    "Single sensor-based approaches have gained significant traction in object detection tasks due to their simplicity and cost-effectiveness. One of the representative models in this category is YOLOv3 proposed by Redmon and Farhadi (2018). This model is an incremental improvement over previous YOLO models, showcasing remarkable performance in real-time object detection tasks.\n",
    "\n",
    "Moreover, the performance of YOLOv8 in pedestrian detection was explored by Sj√∂berg (2023). This study provided crucial insights into the application of single sensor-based approaches in pedestrian detection tasks, one of the most challenging scenarios in autonomous driving.\n",
    "\n",
    "Also noteworthy is the work of Yan et al. (2022), which introduced 2DPASS, a system that utilizes 2D priors for semantic segmentation on lidar point clouds. This method effectively leverages lidar data for object detection, further advancing the capabilities of single sensor-based models.\n",
    "\n",
    "## Multiple Sensor-Based Approaches\n",
    "The fusion of data from multiple sensors has emerged as a promising avenue to improve object detection performance. Broedermann et al. (2023) proposed HRFuser, a multi-resolution sensor fusion architecture for 2D object detection. By leveraging different sensor data at varying resolutions, HRFuser demonstrated a remarkable ability to detect objects under diverse conditions.\n",
    "\n",
    "These studies collectively indicate the substantial potential of both single sensor and multiple sensor-based approaches in tackling the complexities of object detection tasks. As the evolution of sensor technologies continues, we expect further improvements in object detection performance, particularly under challenging conditions such as nighttime and extreme weather scenarios. This underscores the relevance of our current study, which aims to evaluate and compare these approaches across various conditions.\n",
    "\n",
    "\n",
    "### Problem Statements\n",
    "Key challenges that requires to be address by our models are:\n",
    "1. The model requires to handle varying levels of brightness within the image.\n",
    "2. Removing noise from nighttime image, as image taken at night might have more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e923798",
   "metadata": {},
   "source": [
    "# 2. Exploration Analysis or Data or RL Tasks\n",
    "\n",
    "## I. Dataset discovery\n",
    "| Dataset | mmWave Radar Amount | Radar Features | Radar Sensor Amount | Radar Position | Time | Weather | Labelling Box Type | Sensor-LiDAR | Sensor-Camera |\n",
    "| ------- | ------------------- | -------------- | ------------------- | -------------- | ---- | ------- | ------------------ | ------------ | ------------- |\n",
    "| Astyx | 500 | Point cloud | not specify | not specify | day | 1 | 3D | Yes | Yes |\n",
    "| CARRADA | 12600 | Point cloud, Doppler, Range, Azimuth | 1 | front | day | 1 | 2D | No | Yes |\n",
    "| CRUW | 396000 | Point cloud, Doppler, Range, Azimuth | not specify | not specify | day | 1 | Point | No | Yes |\n",
    "| K-Radar | 35000 | Point cloud, Doppler, Range, Azimuth, Elevation | 1 | front | day/night | 5 | 3D | Yes | Yes |\n",
    "| NuScenes | 40000 | Point cloud | 5 | 3 Front + 2 Back radar | day/night | 3 | 3D | Yes | Yes |\n",
    "| RADDet | 10000 | Point cloud, Doppler, Range, Azimuth | 1 | front | day | 1 | 2D | No | Yes |\n",
    "| RADIATE | 44000 | Point cloud, Doppler, Range, Azimuth | 1 | top | day/night | 4 | 2D | Yes | Yes |\n",
    "| VoD | 87000 | Point cloud | 1 | front | day | 1 | 3D | Yes | Yes |\n",
    "| Zendar | 4800 | Point cloud, Doppler, Range, Azimuth | 1 | front | day | 1 | 2D | Yes | Yes |\n",
    "\n",
    "\n",
    "\n",
    "Notes: all the dataset links are in the reference section.\n",
    "\n",
    "The nuScenes dataset is chosen for the experiment for the following reasons:\n",
    "\n",
    "- **Popular in Research Area**: The nuScenes dataset is highly recognized with 3104 citations, reflecting its extensive usage in road object detection research.\n",
    "\n",
    "- **Day & Night Samples**: Including both day and night conditions ensures a wide variety of scenarios, enhancing the robustness of the experiment.\n",
    "\n",
    "- **Sensor-Riched**: Featuring data from LiDAR, radar, and cameras provides a comprehensive view, allowing for sophisticated analysis and improved accuracy.\n",
    "\n",
    "- **Diverse and Realistic Scenarios**: Comprising diverse driving scenarios from real-world environments offers a realistic understanding of real-world driving situations.\n",
    "\n",
    "- **Well-Documented and Structured**: The well-organized and detailed annotations facilitate efficient preprocessing and more focused model development.\n",
    "\n",
    "- **Inclusion of Adverse Weather Conditions**: Scenarios with rain and fog add additional complexity vital for developing weather-robust algorithms.\n",
    "\n",
    "Overall, the nuscenes dataset is the ideal option for our project.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. Data preprocessing\n",
    "Nuscenes v1.0 https://www.nuscenes.org/nuscenes\n",
    "23 category, 6975 instance, 12 sensor,\n",
    "3977 samples\n",
    "\n",
    "![data sample](images\\preprocessing\\nuscenes_dataset_samples_1.png)\n",
    "![data sample](images\\preprocessing\\nuscenes_dataset_samples_2.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " - Data preprocessing tasks:\n",
    "   - Simplified the classes (23 cls -> 3 cls)\n",
    "   - 3D bounding box to 2D bounding box\n",
    "   - Labelling the day/night"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 classes\n",
      "human class: ['human.pedestrian.adult', 'human.pedestrian.child', 'human.pedestrian.wheelchair', 'human.pedestrian.stroller', 'human.pedestrian.personal_mobility', 'human.pedestrian.police_officer', 'human.pedestrian.construction_worker']\n",
      "vehicle class: ['vehicle.car', 'vehicle.bus.bendy', 'vehicle.bus.rigid', 'vehicle.truck', 'vehicle.construction', 'vehicle.emergency.ambulance', 'vehicle.emergency.police', 'vehicle.trailer']\n",
      "bicycle class: ['vehicle.motorcycle', 'vehicle.bicycle', 'static_object.bicycle_rack']\n"
     ]
    }
   ],
   "source": [
    "# Simplified the classes (23 -> 3)\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "nusc = NuScenes(dataroot=r'D:\\Projects\\onRoadDatasets\\StreamPETR\\data\\sets\\nuscenes',  verbose=False)\n",
    "cat = []\n",
    "for i in range(len(nusc.category)):\n",
    "    # print(nusc.category[i]['name'])\n",
    "    cat.append(nusc.category[i]['name'])\n",
    "print(f\"There are {len(cat)} classes\")\n",
    "\n",
    "class_human = [i for i in cat if \"human\" in i]\n",
    "class_bicycle = [i for i in cat if \"cycle\" in i]\n",
    "class_vehicle = [i for i in cat if \"vehicle\" in i and i not in class_bicycle]\n",
    "print(f\"human class: {class_human}\")\n",
    "print(f\"vehicle class: {class_vehicle}\")\n",
    "print(f\"bicycle class: {class_bicycle}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "classes that we are detecting :\n",
    "\n",
    "- human\n",
    "- car\n",
    "- bicycle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Day/Night Labelling and 3D bounding box to 2D bounding box\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample annotations: 2342\n",
      "Day/Night samples: 1638 / 704\n"
     ]
    }
   ],
   "source": [
    "out_sample_path = set()\n",
    "out_sample_anno = dict()\n",
    "out_sample_scene = dict()\n",
    "out_sample_is_day = dict()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    my_scene = nusc.scene[i]\n",
    "    is_day = False if \"night\" in my_scene['description'].lower() else True\n",
    "\n",
    "    first_sample_token = my_scene['first_sample_token']\n",
    "    my_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "    front_cam_sample = nusc.get('sample_data', my_sample['data']['CAM_FRONT'])\n",
    "    out_sample_path.add(front_cam_sample['filename'])\n",
    "    # get annotation\n",
    "    out_sample_anno[front_cam_sample['filename']] = nusc.get_boxes(front_cam_sample['token'])\n",
    "    out_sample_is_day[front_cam_sample['filename']] = is_day\n",
    "    out_sample_scene[front_cam_sample['filename']] = i\n",
    "\n",
    "    while front_cam_sample['next']:\n",
    "        front_cam_sample = nusc.get('sample_data', front_cam_sample['next'])\n",
    "        out_sample_path.add(front_cam_sample['filename'])\n",
    "        # get annotation\n",
    "        out_sample_anno[front_cam_sample['filename']] = nusc.get_boxes(front_cam_sample['token'])\n",
    "        out_sample_is_day[front_cam_sample['filename']] = is_day\n",
    "        out_sample_scene[front_cam_sample['filename']] = i\n",
    "\n",
    "\n",
    "out_sample_anno_json = {k:str(v) for k,v in out_sample_anno.items()}\n",
    "\n",
    "import json\n",
    "with open('preprocessing/front_cam_sample_path.json', 'w+') as file:\n",
    "    json.dump(list(out_sample_path), file)\n",
    "\n",
    "with open('preprocessing/front_cam_sample_anno.json', 'w+') as file:\n",
    "    json.dump(out_sample_anno_json, file)\n",
    "\n",
    "with open('preprocessing/front_cam_sample_scene.json', 'w+') as file:\n",
    "    json.dump(out_sample_scene, file)\n",
    "\n",
    "with open('preprocessing/front_cam_sample_is_day.json', 'w+') as file:\n",
    "    json.dump(out_sample_is_day, file)\n",
    "\n",
    "\n",
    "print(f\"Sample annotations: {len(out_sample_anno)}\")\n",
    "print(f\"Day/Night samples: {sum(out_sample_is_day.values())} / {len(out_sample_is_day.keys()) - sum(out_sample_is_day.values())}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2D bounding box generation\n",
    "Converting 3D bounding box to 2D via [the code](preprocessing/class_simplified_and_2D_BB_generation.ipynb)\n",
    "![bb_2d_to_3d](images/preprocessing/3d_to_2d_bb.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applying radar point cloud to 2D image\n",
    "Refer to [the code](preprocessing/nuscenes_viz.py), we can easily apply the 3D point cloud data from radar/lidar to the 2D images, here are some examples:\n",
    "First column: camera\n",
    "Second column: camera+lidar point cloud\n",
    "Third column: camera+radar point cloud\n",
    "![point_cloud_to_img_1](images/preprocessing/apply_point_cloud_to_img_1.png)\n",
    "![point_cloud_to_img_2](images/preprocessing/apply_point_cloud_to_img_2.png)\n",
    "![point_cloud_to_img_3](images/preprocessing/apply_point_cloud_to_img_3.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "4f399faf",
   "metadata": {},
   "source": [
    "# 3. Models\n",
    "We have thoroughly examined four distinct models - one baseline model and three alternative designs - in order to compare their performance based on variations in network architecture and input features. Our primary objective is to discern the most critical factors that can enhance the accuracy of nighttime object detection for deep learning models. All the models we explored are sourced from research papers published in recent years. Additionally, we have endeavored to fine-tune the parameters of each model to optimize them specifically for nighttime detection.\n",
    "\n",
    "| Networks | Year | Sensor | Nighttime Object Detection Capabilities                                                                                                                                           |\n",
    "| -------- | ---- | ------ |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| YOLOv3   | 2019 | Camera | famous and popular models, real-time detection capability can be advantageous for dynamic driving scenarios.                                                                      |\n",
    "| YOLOv8s  | 2023 | Camera | Improved detection accuracy over YOLOv3. Its suitability for embedded devices enables on-board processing, beneficial for real-time applications.                                 |\n",
    "| HRFuser  | 2022 | Camera, LiDAR, Radar | Excellent performance in low-light and foggy situations due to the fusion of different sensor data, thus, highly reliable for nighttime on-road object detection.                 |\n",
    "| 2DPASS   | 2022 | Camera and LiDAR | Top model on the leaderboard, effective for nighttime detection due to the fusion of LiDAR and camera data, offering depth perception that is beneficial in low-light conditions. |\n",
    "\n",
    "\n",
    "\n",
    "## YOLOv3\n",
    "Redmon, J. and Farhadi, A., 2018. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767.\n",
    " - baseline model: well-known architecture\n",
    " - real-time detection\n",
    " - Based on camera img only\n",
    " - 106 layers (darknet 53 + producing 53)\n",
    "\n",
    "\n",
    "\n",
    "## YOLOv8s\n",
    "Link to document: https://docs.ultralytics.com/\n",
    "\n",
    "#### Model Introduction\n",
    "\n",
    "The state-of-the-art object detection system YOLO (You Only Look Once) is a single-stage detector.Since the initial release of YOLOv1 in 2016, the YOLO family has been refined and upgraded to YOLOv8. We chose YOLOv8, the most recent iteration of the YOLO series, to conduct 2D object detection for this project. Instead of a single model, YOLOv8 has created multiple versions, each with its own distinctive characteristics. Our project uses YOLOv8s. These version are:\n",
    "1. YOLOv8n ---- The nano version\n",
    "2. YOLOv8s ---- The small version\n",
    "3. YOLOv8m ---- The medium version\n",
    "4. YOLOv8l ---- The large version\n",
    "The performance of recent version of YOLO is shown below:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8 performance.png\" />\n",
    "</div>\n",
    "refer to https://github.com/ultralytics/ultralytics\n",
    "\n",
    "\n",
    "Architecture of YOLOv8 is shown below:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8 network.jpeg\" />\n",
    "</div>\n",
    "refer to https://arxiv.org/pdf/2304.00501.pdf\n",
    "\n",
    "\n",
    "\n",
    "Compared with previous YOLO algorithms, YOLOv8 has a new backnone network, a new anchor-free detection head and a new loss function. It uses a similar backbone network as YOLOv5 but changes CSPLayer to C2f which combines high-level features with contextual information to improve detection accuracy. The anchor-free detection head allows each branch to focus on its own tasks and improve overall accuracy. The new loss function uses CIoU and DFL loss function for bounding box loss and binary cross-entropy for classification loss, which helps to improve detection performance , especially when detecting small objects.\n",
    "\n",
    "## HRFUser\n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/2206.15157.pdf\n",
    "\n",
    "\n",
    "\n",
    "#### Model Introduction\n",
    "\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "While numerous recent research focus on fusing specific pairs of sensors‚Äîsuch as camera with lidar or radar‚Äîby leveraging architectural components relevant to the investigated context, the literature lacks a general and modular sensor fusion architecture. We have¬†HRFuser, a modular architecture for multi-modal 2D object identification. It multiresolutionly integrates numerous sensors and scales to an indefinite number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "HRFuser have a slight special architecture being shown as follow:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-architecture.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "Because of extended layer of input, HRUser results in a better training of combination Data on not just cameras but also multiple type of sensors\n",
    "\n",
    "### 2DPASS \n",
    "Link to paper: https://arxiv.org/pdf/2207.04397.pdf \n",
    "\n",
    "#### Model Introduction\n",
    "2DPASS (2D Priors Assisted Semantic Segmentation) leverages an auxiliary modal fusion and multi-scale fusion-to-single knowledge distillation (MSFSKD) to acquire richer semantic and structural information from multi-modal data consisting of 2D camera images and 3D LiDAR point clouds.\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/2DPASS_camera_lidar.png\" />\n",
    "</div>\n",
    "\n",
    "LiDAR offers accurate depth information regardless of lighting conditions so it is not inhibited by dark conditions and will be able to effectively assist in day/night object detection. However, the data that LiDAR captures is sparse and textureless and so may benefit from additional sources of data. Cameras images are able to provide dense color information and fine grained textures but provide ambiguous depth perception and can be unclear in low light conditions such as at night. As cameras and LiDAR complement each other, 2DPASS was chosen as it uses both input modalities and we check if this provides better night time detection compared to models only using 2D images.\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/2DPASS_model_structure.png\" />\n",
    "</div>\n",
    "\n",
    "The model uses two independent networks to extract multi-scale features from the 2D images and 3D point clouds in parallel. It has a 2D network which takes in a small patch (480 x 320) from the full camera image as the input and applies a ResNet34 encoder with 2D convolutions. The 3D network takes in 3D point clouds and uses sparse convolutions to take advantage of the sparse nature of the point cloud data.\n",
    " \n",
    "Multi scale fusion to single knowledge distillation (MSFSKD) then enhances the 3D network by providing textural information and structural regularisation from the 2D network to enhance feature learning. To transfer information between the two modalities, point-to-pixel correspondence is used to create paired features of the two modalities. The 3D features are transformed using a multilayer perceptron before they are fused with their 2D counterparts and this can be used to improve the 3D network. Then features at each scale are then used to generate the semantic segmentation predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b6c7f",
   "metadata": {},
   "source": [
    "# 4. Model Results\n",
    "## Overall comparison\n",
    "| Networks | Year | Sensor | mAP (day/night) | IoU | Pros | Cons |\n",
    "| -------- | ---- | ------ | -------------- | --- | ---- | ---- |\n",
    "| YOLOv3   | 2019 | Camera | 48/24.1        | 23  | Light weight | Low performance |\n",
    "| YOLOv8s  | 2023 | Camera | 65/18.4        | 30  | Suitable for embed device | Low performance |\n",
    "| HRFuser  | 2022 | Camera, LiDAR, Radar | 51/47 | 79 | High quality detection on low-light and foggy situations | Requires multiple inputs; High computation cost |\n",
    "| 2DPASS   | 2022 | Camera and LiDAR | 66/63 | 81 | Good for nighttime detection | High computation cost |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254fdf1",
   "metadata": {},
   "source": [
    "### YOLOv8s Result\n",
    "#### Model Result\n",
    "According to the table below, YOLOv8s performs better in daytime detection than nighttime detection. For nighttime detection, the mAP is only 18%.\n",
    "\n",
    "| Dataset     | mAP (%)     |\n",
    "|-------------|-------------|\n",
    "|overall      | 40%         |\n",
    "|daytime      | 65%         |\n",
    "|nighttime    | 18%         |\n",
    "\n",
    "Based on the confusion matrix of model on overall dataset shown below, we can see that car has the highest precision, while other categories are easily misdetected as background.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_confusion_matrix.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "#### Prediction Output\n",
    "\n",
    "The comparison between prediction and label shows that although YOLOv8s performs better than YOLOv3, the result is still not ideal due to misdetection.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_prediction.jpeg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_label.jpeg\" width=\"700px\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20579346",
   "metadata": {},
   "source": [
    "#### Model Tuning\n",
    "In order to improve the precision of our model, we tried model tuning to get the best model. We followed the introduction of guides for best training results and hyperparameter evolution.\n",
    "Attachment is:\n",
    "- https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/\n",
    "- https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/\n",
    "Among parameters we adjusted, the effect of image size is one of the greatest. We tried 640 and 1280 respectively.\n",
    "The results of the model are shown below:\n",
    "\n",
    "| imagesize   | mAP (%)     |\n",
    "|-------------|-------------|\n",
    "|640          | 38%         |\n",
    "|1280         | 40%         |\n",
    "Thus, we chose imagesize = 1280, as it has a better result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505ece0",
   "metadata": {},
   "source": [
    "### HRFUser Results\n",
    "\n",
    "#### General sample images results:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser_result.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-Result-2.png\" width=\"450px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Images output from Nuscene MiniDataset after train (note this is only a few out of more than 500 results):\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuser_Result.png\" width=\"750px\" />\n",
    "</div>\n",
    "\n",
    "##### Front Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC1.jpg\" width=\"375px\" />\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC2.jpg\" width=\"375px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC1.jpg\" width=\"375px\" />\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC2.jpg\" width=\"375px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Left Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC1.jpg\" width=\"375px\" />\n",
    "    <img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC2.jpg\" width=\"375px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Right Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR_Camera2.jpg\" width=\"375px\" />\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR-Camera1.jpg\" width=\"375px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72d0a2",
   "metadata": {},
   "source": [
    "Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cc86a",
   "metadata": {},
   "source": [
    "### 2DPASS Results\n",
    "\n",
    "#### 2DPASS Trained on Mini-Dataset\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/mini.png\" width=\"850px\" />\n",
    "</div>\n",
    "\n",
    "#### 2DPASS Pretrained Model\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/pretrained.png\" width=\"850px\" />\n",
    "</div>\n",
    "\n",
    "#### Model Results (organised into a table)\n",
    "**Per Class IoU**\n",
    "\n",
    "| Class                         | IoU Our Training (%) | IoU Pretrained (%) |\n",
    "|-------------------------------|----------|----------|\n",
    "| Movable Object Barrier        |  0.00    |  84.60   |\n",
    "| Bicycle               |  0.00    |  64.95   |\n",
    "| Bus             |  23.21   |  92.30   |\n",
    "| Car                   |  84.27   |  90.76   |\n",
    "| Construction          |   NaN    |   NaN    |\n",
    "| Motorcycle            |  0.28    |  71.84   |\n",
    "| Human Pedestrian Adult        |  44.91   |  86.12   |\n",
    "| Movable Object Trafficcone    |  0.00    |  47.46   |\n",
    "| Trailer               |  0.00    |  86.06   |\n",
    "| Truck                 |  66.60   |  71.99   |\n",
    "| Driveable Surface        |  89.26   |  96.10   |\n",
    "| Other                    |  1.19    |  85.00   |\n",
    "| Sidewalk                 |  33.79   |  72.88   |\n",
    "| Terrain                  |  59.13   |  86.08   |\n",
    "| Manmade                |  72.33   |  91.41   |\n",
    "| Vegetation             |  69.03   |  91.09   |\n",
    "\n",
    "**Global Metrics**\n",
    "\n",
    "| Metric       | Our Training           | Pretrained              |\n",
    "|--------------|------------------|---------------------|\n",
    "| Accuracy      | 0.56             | 0.63                |\n",
    "| mIoU     | 0.36             | 0.81                |\n",
    "\n",
    "Major improvements in accuracy and mIoU are both significant for the pretrained model which was initially trained on the full dataset. Note, that this result is worse than the one displayed in the paper as their model was trained with additional validation set and using instance-level augmentation.\n",
    "\n",
    "#### Epoch Training Steps\n",
    "NOTE: X-axis is number of epoch.\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center; margin-bottom: 30px;\">\n",
    "  <div style=\"text-align: center; margin-right: 5px;\">\n",
    "    <h4>mIoU vs Epoch</h4>\n",
    "    <img src=\"./images/miou_r.png\" alt=\"mIoU vs Epoch\" width=\"500px\" style=\"margin-bottom: 20px;\" />\n",
    "  </div>\n",
    "  <div style=\"text-align: center; margin-left: 5px;\">\n",
    "    <h4>Best mIoU vs Epoch</h4>\n",
    "    <img src=\"./images/miou.png\" alt=\"Best mIoU vs Epoch\" width=\"500px\" style=\"margin-bottom: 20px;\" />\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "From the mIoU curves and best mIoU curve(smoothened out), we see that around 8000 epoch there are no significant improves in the mIoU value, emphasizing that further training after 8000 epoch does not improve the model, and could lead to overfitting existing data.\n",
    "\n",
    "##### Accuracy vs Epoch \n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/accuracy.png\" width=\"500px\" />\n",
    "</div>\n",
    "The accuracy during the training of the model behaves similarly to the mIoU curve as optimum accuracy is reached around 8000 epoche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b040a",
   "metadata": {},
   "source": [
    "#### 2DPASS Model Tuning\n",
    "To improve the existing model we have tried hyper-parameter tuning. We did this by incrementing and decrementing its parameters, and in particular:\n",
    "- Learning Rate (original parameter 0.24)\n",
    "- Optimizer (tried Adam instead of SGD)\n",
    "- Momentum (original momentum 0.9)\n",
    "By introducing small increments and decrements, we retrained the model on the mini-dataset. However, we only trained this twice and used the following paramters:\n",
    "- Learning Rate (0.20)\n",
    "- Optimizer (Adam)\n",
    "- Momentum (original momentum 0.85)\n",
    "\n",
    "Results:\n",
    "\n",
    "| Metric       | Our Training           | Original Training                |\n",
    "|--------------|------------------|---------------------|\n",
    "| Accuracy      | 0.41             | 0.56                |\n",
    "| mIoU     | 0.24             | 0.36                |\n",
    "\n",
    "and;\n",
    "- Learning Rate (0.3)\n",
    "- Optimizer (Adam)\n",
    "- Momentum (original momentum 0.95)\n",
    "\n",
    "Results:\n",
    "\n",
    "| Metric       | Our Training           | Original Training              |\n",
    "|--------------|------------------|---------------------|\n",
    "| Accuracy      | 0.46             | 0.56               |\n",
    "| mIoU     | 0.29             | 0.36                |\n",
    "\n",
    "However, our results became worse for both runs, using learning rates of 0.20 and 0.3, Adam optimizer, and momemtums of 0.85 and 0.95 respectively. This suggests that the original hyperparameters were already near optimal for this model. We also highly doubt that using a smaller training dataset would have impacted this, but we did consider that possibility.\n",
    "Further hyper-paramter tuning could not be tested due to time limitations, as each run of the training took 5 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6353506",
   "metadata": {},
   "source": [
    "# Model Code\n",
    "Our experimental setup involved careful parameter tuning, meticulous selection of evaluation metrics, and strategic data splitting for training, validation, and testing. YOLOv8s was trained using SGD optimizer with a learning rate of 0.01, a batch size of 16, and an IoU threshold of 0.7. For HRFuser, due to hardware constraints, we used a batch size of 1 and made use of pre-trained weights. We also used a mini-training dataset of Nuscenes due to computational and time constraints. Lastly, 2DPASS was trained with a learning rate of 0.24, SGD optimizer, momentum of 0.9, and weight decay of 1.0e-4. A batch size of 1 was adopted due to memory constraints. Evaluation metrics for all models included mAP (mean average precision) for object detection, evaluated separately for day and night conditions, and IoU (Intersection over Union) for bounding box overlap. For training and validation, we used a subset of the nuScenes dataset, ensuring a balanced representation of different classes and illumination conditions (day and night). The test set was kept separate and was not involved in any part of the model training or tuning process. Codes are as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "au9g0521",
   "metadata": {},
   "source": [
    "## Yolov8 Code\n",
    "### Experiment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "as580627",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/dataset\n",
    "%cd /content/dataset\n",
    "!pip install roboflow\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key='U9qUuMcwNCEJrQCd2sUC')\n",
    "project = rf.workspace('unsw-kgzbp').project('comp9444-v2')\n",
    "dataset = project.version(1).download('yolov8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agj02856",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics==8.0.20\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "import ultralytics\n",
    "import time\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca58aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=150 imgsz=1280 plots=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kl062697",
   "metadata": {},
   "source": [
    "The result is shown below\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8 result.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d337000",
   "metadata": {},
   "source": [
    "## 2DPASS Code with Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631464cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samyu\\Code\\comp9444project\\2DPASS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/samyu/Code/comp9444project/2DPASS\")\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import datetime\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from easydict import EasyDict\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.profiler import SimpleProfiler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, StochasticWeightAveraging\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from dataloader.dataset import get_model_class, get_collate_class\n",
    "from dataloader.pc_dataset import get_pc_model_class\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from torch import distributed as dist\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20734da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 1, 'model_params': {'model_architecture': 'arch_2dpass', 'input_dims': 4, 'spatial_shape': [1000, 1000, 70], 'scale_list': [2, 4, 8, 16, 16, 16], 'hiden_size': 256, 'num_classes': 17, 'backbone_2d': 'resnet34', 'pretrained2d': False}, 'dataset_params': {'training_size': 28130, 'dataset_type': 'point_image_dataset_nus', 'pc_dataset_type': 'nuScenes', 'collate_type': 'collate_fn_default', 'ignore_label': 0, 'label_mapping': './config/label_mapping/nuscenes.yaml', 'resize': [400, 240], 'color_jitter': [0.4, 0.4, 0.4], 'flip2d': 0.5, 'image_normalizer': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'max_volume_space': [50, 50, 3], 'min_volume_space': [-50, -50, -4], 'train_data_loader': {'data_path': './dataset/nuscenes/', 'batch_size': 1, 'shuffle': True, 'num_workers': 8, 'rotate_aug': True, 'flip_aug': True, 'scale_aug': True, 'transform_aug': True, 'dropout_aug': True}, 'val_data_loader': {'data_path': './dataset/nuscenes', 'shuffle': False, 'num_workers': 8, 'batch_size': 1, 'rotate_aug': False, 'flip_aug': False, 'scale_aug': False, 'transform_aug': False, 'dropout_aug': False}}, 'train_params': {'max_num_epochs': 80, 'learning_rate': 0.24, 'optimizer': 'SGD', 'lr_scheduler': 'CosineAnnealingLR', 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001, 'lambda_seg2d': 1, 'lambda_xm': 0.05, 'lambda_lovasz': 1}, 'gpu': [0], 'seed': 0, 'config_path': 'config/2DPASS-nuscenese.yaml', 'log_dir': 'default', 'monitor': 'val/mIoU', 'stop_patience': 50, 'save_top_k': 1, 'check_val_every_n_epoch': 1, 'SWA': False, 'baseline_only': False, 'test': True, 'fine_tune': False, 'pretrain2d': False, 'num_vote': 1, 'submit_to_server': False, 'checkpoint': None, 'debug': False}\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "34149 lidarseg,\n",
      "Done loading in 1.859 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Total 162 scenes in the val split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "def load_yaml(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        try:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except:\n",
    "            config = yaml.load(f)\n",
    "    return config\n",
    "\n",
    "def parse_config():\n",
    "    config = load_yaml('config/2DPASS-nuscenese.yaml')  # Load config from yaml file\n",
    "\n",
    "    # manually set the values that were previously command-line arguments\n",
    "    config['gpu'] = [0]\n",
    "    config['seed'] = 0\n",
    "    config['config_path'] = 'config/2DPASS-nuscenese.yaml'\n",
    "    config['log_dir'] = 'default'\n",
    "    config['monitor'] = 'val/mIoU'\n",
    "    config['stop_patience'] = 50\n",
    "    config['save_top_k'] = 1\n",
    "    config['check_val_every_n_epoch'] = 1\n",
    "    config['SWA'] = False\n",
    "    config['baseline_only'] = False\n",
    "    config['test'] = True\n",
    "    config['fine_tune'] = False\n",
    "    config['pretrain2d'] = False\n",
    "    config['num_vote'] = 1\n",
    "    config['submit_to_server'] = False\n",
    "    config['checkpoint'] = None\n",
    "    config['debug'] = False\n",
    "\n",
    "    return EasyDict(config)\n",
    "\n",
    "\n",
    "def build_loader(config):\n",
    "    pc_dataset = get_pc_model_class(config['dataset_params']['pc_dataset_type'])\n",
    "    dataset_type = get_model_class(config['dataset_params']['dataset_type'])\n",
    "    train_config = config['dataset_params']['train_data_loader']\n",
    "    val_config = config['dataset_params']['val_data_loader']\n",
    "    train_dataset_loader, val_dataset_loader, test_dataset_loader = None, None, None\n",
    "\n",
    "    if not config['test']:\n",
    "        train_pt_dataset = pc_dataset(config, data_path=train_config['data_path'], imageset='train')\n",
    "        val_pt_dataset = pc_dataset(config, data_path=val_config['data_path'], imageset='val')\n",
    "        train_dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_type(train_pt_dataset, config, train_config),\n",
    "            batch_size=train_config[\"batch_size\"],\n",
    "            collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "            shuffle=train_config[\"shuffle\"],\n",
    "            num_workers=train_config[\"num_workers\"],\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        # config['dataset_params']['training_size'] = len(train_dataset_loader) * len(configs.gpu)\n",
    "        val_dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_type(val_pt_dataset, config, val_config, num_vote=1),\n",
    "            batch_size=val_config[\"batch_size\"],\n",
    "            collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "            shuffle=val_config[\"shuffle\"],\n",
    "            pin_memory=True,\n",
    "            num_workers=val_config[\"num_workers\"]\n",
    "        )\n",
    "    else:\n",
    "        if config['submit_to_server']:\n",
    "            test_pt_dataset = pc_dataset(config, data_path=val_config['data_path'], imageset='test', num_vote=val_config[\"batch_size\"])\n",
    "            test_dataset_loader = torch.utils.data.DataLoader(\n",
    "                dataset=dataset_type(test_pt_dataset, config, val_config, num_vote=val_config[\"batch_size\"]),\n",
    "                batch_size=val_config[\"batch_size\"],\n",
    "                collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "                shuffle=val_config[\"shuffle\"],\n",
    "                num_workers=val_config[\"num_workers\"]\n",
    "            )\n",
    "        else:\n",
    "            val_pt_dataset = pc_dataset(config, data_path=val_config['data_path'], imageset='val', num_vote=val_config[\"batch_size\"])\n",
    "            val_dataset_loader = torch.utils.data.DataLoader(\n",
    "                dataset=dataset_type(val_pt_dataset, config, val_config, num_vote=val_config[\"batch_size\"]),\n",
    "                batch_size=val_config[\"batch_size\"],\n",
    "                collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "                shuffle=val_config[\"shuffle\"],\n",
    "                num_workers=val_config[\"num_workers\"]\n",
    "            )\n",
    "\n",
    "    return train_dataset_loader, val_dataset_loader, test_dataset_loader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parameters\n",
    "    configs = parse_config()\n",
    "    print(configs)\n",
    "\n",
    "    # setting\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, configs.gpu))\n",
    "    num_gpu = len(configs.gpu)\n",
    "\n",
    "    # output path\n",
    "    log_folder = 'logs/' + configs['dataset_params']['pc_dataset_type']\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(log_folder, name=configs.log_dir, default_hp_metric=False)\n",
    "    os.makedirs(f'{log_folder}/{configs.log_dir}', exist_ok=True)\n",
    "    profiler = SimpleProfiler(output_filename=f'{log_folder}/{configs.log_dir}/profiler.txt')\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    # save the backup files\n",
    "    backup_dir = os.path.join(log_folder, configs.log_dir, 'backup_files_%s' % str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')))\n",
    "    if not configs['test']:\n",
    "        os.makedirs(backup_dir, exist_ok=True)\n",
    "        os.system('copy main.py {}'.format(backup_dir))\n",
    "        os.system('copy dataloader/dataset.py {}'.format(backup_dir))\n",
    "        os.system('copy dataloader/pc_dataset.py {}'.format(backup_dir))\n",
    "        os.system('copy {} {}'.format(configs.config_path, backup_dir))\n",
    "        os.system('copy network/base_model.py {}'.format(backup_dir))\n",
    "        os.system('copy network/baseline.py {}'.format(backup_dir))\n",
    "        os.system('copy {}.py {}'.format('network/' + configs['model_params']['model_architecture'], backup_dir))\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(configs.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    np.random.seed(configs.seed)\n",
    "    config_path = configs.config_path\n",
    "\n",
    "    train_dataset_loader, val_dataset_loader, test_dataset_loader = build_loader(configs)\n",
    "    model_file = importlib.import_module('network.' + configs['model_params']['model_architecture'])\n",
    "    my_model = model_file.get_model(configs)\n",
    "\n",
    "    pl.seed_everything(configs.seed)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=configs.monitor,\n",
    "        mode='max',\n",
    "        save_last=True,\n",
    "        save_top_k=configs.save_top_k)\n",
    "\n",
    "    if configs.checkpoint is not None:\n",
    "        print('load pre-trained model...')\n",
    "        if configs.fine_tune or configs.test or configs.pretrain2d:\n",
    "            my_model = my_model.load_from_checkpoint(configs.checkpoint, config=configs, strict=(not configs.pretrain2d))\n",
    "        else:\n",
    "            # continue last training\n",
    "            my_model = my_model.load_from_checkpoint(configs.checkpoint)\n",
    "\n",
    "    if configs.SWA:\n",
    "        swa = [StochasticWeightAveraging(swa_epoch_start=configs.train_params.swa_epoch_start, annealing_epochs=1)]\n",
    "    else:\n",
    "        swa = []\n",
    "\n",
    "    print('Start testing...')\n",
    "    assert num_gpu == 1, 'only support single GPU testing!'\n",
    "    trainer = pl.Trainer(gpus=[i for i in range(num_gpu)],\n",
    "                         accelerator='ddp_spawn',\n",
    "                         resume_from_checkpoint=configs.checkpoint,\n",
    "                         logger=tb_logger,\n",
    "                         profiler=profiler)\n",
    "\n",
    "    trainer.test(my_model, test_dataset_loader if configs.submit_to_server else val_dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85e4f0",
   "metadata": {},
   "source": [
    "Upon running the above code, the training would actually be executed in the temrinal, and I have attached the respective photos below:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/2DPASS Jup_training.png\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/pretrained jup.png\" />\n",
    "</div>\n",
    "\n",
    "This result was analysed in the results section above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6e46b",
   "metadata": {},
   "source": [
    "## HRFuser code demo on a pre trained weight with output file generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce02e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.utils import get_git_hash\n",
    "\n",
    "from mmdet import __version__\n",
    "from mmdet.apis import init_random_seed, set_random_seed, train_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.utils import collect_env, get_root_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a527dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "arguments = ['./HRFuser_config/hrfuser/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_bn.py', # batch-norm\n",
    "             './checkpoints/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_latest.pth',\n",
    "             '--cfg-options', 'data.test.samples_per_gpu=1',\n",
    "             '--show-dir', 'demo/output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9284d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a detector')\n",
    "    parser.add_argument('config', help='train config file path')\n",
    "    parser.add_argument('--work-dir', help='the dir to save logs and models')\n",
    "    parser.add_argument(\n",
    "        '--resume-from', help='the checkpoint file to resume from')\n",
    "    parser.add_argument(\n",
    "        '--no-validate',\n",
    "        action='store_true',\n",
    "        help='whether not to evaluate the checkpoint during training')\n",
    "    group_gpus = parser.add_mutually_exclusive_group()\n",
    "    group_gpus.add_argument(\n",
    "        '--gpus',\n",
    "        type=int,\n",
    "        help='number of gpus to use '\n",
    "        '(only applicable to non-distributed training)')\n",
    "    group_gpus.add_argument(\n",
    "        '--gpu-ids',\n",
    "        type=int,\n",
    "        nargs='+',\n",
    "        help='ids of gpus to use '\n",
    "        '(only applicable to non-distributed training)')\n",
    "    parser.add_argument('--seed', type=int, default=None, help='random seed')\n",
    "    parser.add_argument(\n",
    "        '--deterministic',\n",
    "        action='store_true',\n",
    "        help='whether to set deterministic options for CUDNN backend.')\n",
    "    parser.add_argument(\n",
    "        '--options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file (deprecate), '\n",
    "        'change to --cfg-options instead.')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file. If the value to '\n",
    "        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n",
    "        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n",
    "        'Note that the quotation marks are necessary and that no white space '\n",
    "        'is allowed.')\n",
    "    parser.add_argument(\n",
    "        '--launcher',\n",
    "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
    "        default='none',\n",
    "        help='job launcher')\n",
    "    parser.add_argument('--local_rank', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args(arguments)\n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "\n",
    "    if args.options and args.cfg_options:\n",
    "        raise ValueError(\n",
    "            '--options and --cfg-options cannot be both '\n",
    "            'specified, --options is deprecated in favor of --cfg-options')\n",
    "    if args.options:\n",
    "        warnings.warn('--options is deprecated in favor of --cfg-options')\n",
    "        args.cfg_options = args.options\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc47b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "    # set cudnn_benchmark\n",
    "    if cfg.get('cudnn_benchmark', False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # work_dir is determined in this priority: CLI > segment in file > filename\n",
    "    if args.work_dir is not None:\n",
    "        # update configs according to CLI args if args.work_dir is not None\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        # use config filename as default work_dir if cfg.work_dir is None\n",
    "        cfg.work_dir = osp.join('./work_dirs',\n",
    "                                osp.splitext(osp.basename(args.config))[0])\n",
    "    if args.resume_from is not None:\n",
    "        cfg.resume_from = args.resume_from\n",
    "    if args.gpu_ids is not None:\n",
    "        cfg.gpu_ids = args.gpu_ids\n",
    "    else:\n",
    "        cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    if args.launcher == 'none':\n",
    "        distributed = False\n",
    "    else:\n",
    "        distributed = True\n",
    "        init_dist(args.launcher, **cfg.dist_params)\n",
    "        # re-set gpu_ids with distributed training mode\n",
    "        _, world_size = get_dist_info()\n",
    "        cfg.gpu_ids = range(world_size)\n",
    "\n",
    "    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "    # create work_dir\n",
    "    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "    # dump config\n",
    "    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n",
    "    # init the logger before other steps\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
    "    logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
    "\n",
    "    # init the meta dict to record some important information such as\n",
    "    # environment info and seed, which will be logged\n",
    "    meta = dict()\n",
    "    # log env info\n",
    "    env_info_dict = collect_env()\n",
    "    env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n",
    "    dash_line = '-' * 60 + '\\n'\n",
    "    logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "                dash_line)\n",
    "    meta['env_info'] = env_info\n",
    "    meta['config'] = cfg.pretty_text\n",
    "    # log some basic info\n",
    "    logger.info(f'Distributed training: {distributed}')\n",
    "    logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "    # set random seeds\n",
    "    if 'seed' in cfg.keys() and cfg.seed is not None:\n",
    "        seed = cfg.seed\n",
    "    else:\n",
    "        seed = init_random_seed(args.seed)\n",
    "    logger.info(f'Set random seed to {seed}, '\n",
    "                f'deterministic: {args.deterministic}')\n",
    "    #set_random_seed(seed, deterministic=args.deterministic)\n",
    "    cfg.seed = seed\n",
    "    meta['seed'] = seed\n",
    "    meta['exp_name'] = osp.basename(args.config)\n",
    "\n",
    "    model = build_detector(\n",
    "        cfg.model,\n",
    "        train_cfg=cfg.get('train_cfg'),\n",
    "        test_cfg=cfg.get('test_cfg'))\n",
    "    model.init_weights()\n",
    "\n",
    "    datasets = [build_dataset(cfg.data.train)]\n",
    "    if len(cfg.workflow) == 2:\n",
    "        val_dataset = copy.deepcopy(cfg.data.val)\n",
    "        val_dataset.pipeline = cfg.data.train.pipeline\n",
    "        datasets.append(build_dataset(val_dataset))\n",
    "    if cfg.checkpoint_config is not None:\n",
    "        # save mmdet version, config file content and class names in\n",
    "        # checkpoints as meta data\n",
    "        cfg.checkpoint_config.meta = dict(\n",
    "            mmdet_version=__version__ + get_git_hash()[:7],\n",
    "            config=cfg.pretty_text,\n",
    "            CLASSES=datasets[0].CLASSES)\n",
    "\n",
    "    # add an attribute for visualization convenience\n",
    "    model.CLASSES = datasets[0].CLASSES\n",
    "    train_detector(\n",
    "        model,\n",
    "        datasets,\n",
    "        cfg,\n",
    "        distributed=distributed,\n",
    "        validate=(not args.no_validate),\n",
    "        timestamp=timestamp,\n",
    "        meta=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d9427",
   "metadata": {},
   "source": [
    "# 5. Discussion\n",
    "\n",
    "### YOLOv8s Discussion\n",
    "#### System Performance\n",
    "System Specifications: We have trained YOLOv8 model through Google Colab with NVIDIA A100-SXM4-40GB\n",
    "\n",
    "#### Training Specifications\n",
    "Training parameter have been pre-tuned by the developer:\n",
    "- Optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n",
    "- batch: 16\n",
    "- IoU: 0.7\n",
    "And we set image size = 1280, which has a better result.\n",
    "\n",
    "#### Training Time\n",
    "The training process of YOLOv8 on the mini dataset would take 0.230 hours. When we added epoch size to 150, our model stopped training at epoch 75, since there was no improvement in the last 50 epochs.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_training _time.png\" />\n",
    "</div>\n",
    "\n",
    "And the best result was observed at epoch 25, so we used epoch size = 25. As a result, the training time has decreased to 0.08 hours.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_training_time_25_epoches.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "The nuImages sample dataset contains approximately 40,000 images in total. But after downloading, we got around 20,000 images as training set. Using all of these images for training the model would result in excessively long training times. Apart from the provided all_sample dataset, the official source also offers a mini dataset with only 50 images. However, if we use the mini dataset to train the model, the small sample size and insufficient representation of certain categories could lead to inaccurate learning for those categories and over-fitting as well.\n",
    "\n",
    "In order to mitigate the challenges associated with dataset size, we constructed a dataset by randomly sampling 350 images from each camera and manually adjusted the distribution of daytime and nighttime images within our mini dataset from nuImage. However, this approach may still lead to imbalanced proportions of different classes in comparison to the full dataset, potentially influencing our prediction results.\n",
    "\n",
    "### HRFUser Model Discussion\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We train this data on another machine which is a Linux-Sub-System Machine but does not have GPU onboard, the training has to be on CPU. We have to reduce the batch size and the dataset. The training would took ages for this to happen so we have to make use of pre-trained weighted\n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of Nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "As we have insufficient training resources, for this device working on this model, we have to make use of the pre-trained weight provided by the Research Paper\n",
    "\n",
    "#### Model Architecture\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "#### Training Time\n",
    "It would take ages, approximately more than 8 hours on the device working on this. But to use pre-trained weight, it would cost us approximately an hour to perform.\n",
    "\n",
    "<div style=\"margin-top: 10px; margin-bottom: 10px;\">\n",
    "<img src=\"./images/train_time_HR_Fuser.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "The challenge for this model is that it makes use of the mmdet library and mmcv but the running environment on the paper provided is only suitable with a Linux running environment. We would have to create a WSL ( Window Subsystem for Linux) and then run the project on it. Also, WSL cannot connect and refer directly with the Window's CUDA and GPU, we have to modify the code for it to accept CPU train on Pytorch. As CPU train is very limited, we have to fully reduced the batch size to 1 and then perform training on a small MiniDataset instead of a huge one. Also, making use of a pre-trained weight would save us much time rather than training the whole process.\n",
    "\n",
    "Overall HRuser actually better the performance on low-light detection since it was able to fuse all the data input such as camera and sensors together for the train. Thus, improve significantly the detection on varies foggy and low-light environment.\n",
    "\n",
    "<div style=\"margin-top: 10px; margin-bottom: 10px;\">\n",
    "<img src=\"./images/HR_Fuser_discussion.png\" />\n",
    "</div>\n",
    "\n",
    "### 2DPASS Discussion\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We have trained the 2DPASS model on a Nvidia 4060 laptop graphics card with 16 gigabytes of RAM. \n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "Training parameters have been pre-tuned by the developers as:\n",
    "- Learning Rate: 0.24\n",
    "- Optimizer: SGD\n",
    "- Momentum: 0.9\n",
    "- Weight Decay: 1.0e-4\n",
    "\n",
    "#### Model Architecture\n",
    "This model significantly improves upon simple image computer vision neural networks, as 2DPASS introduces lidar detection combined with the use of image. This more accurately detects the existence and classification of the object even in low luminosity environments.\n",
    "\n",
    "#### Training Time\n",
    "The training process of our model on the mini-dataset took approximately 5 hours, which is due to our computer‚Äôs limited memory as it was only able to manage a batch training size of one. Also, due to the limited variety in the mini-dataset, we observed that the val/mIoU failed to show improvements over the last 50 records, which shows that a lot of the computation towards the end of training did not achieve any notable performance improvements.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/train_time.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "Originally running the model on the whole 80 gigabytes data requires too much computational power and time, so we resorted to using the mini-training set instead, which was much faster to train.\n",
    "\n",
    "Training on a much smaller dataset could potentially introduce overfitting of data and lead to inaccurate results, in this case we have used their pre-trained model to compare results before drawing conclusions.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/overfit.png\" />\n",
    "</div>\n",
    "The above is the result from testing the model trained with the mini-dataset, and here we can clearly see a case of overfitting where all vehicle like objects are recognised as cars explaining the high accuracy in car predictions and basically 0% accuracy in all other vehicles detections.\n",
    "\n",
    "Our main challenges occurred within our limited ability to modify the model, as the training time even on a much smaller dataset took up to five hours. To tackle this problem, we have introduced early-stopping of the training, where if we do not see noticeable improvements on the mIoU(mean intersection over Union) value over five epochs of training we will manually exit the training. However, finding a sweet spot for the improvement was difficult and is hard to optimise. Moreover, as training is also dependent on the distribution of the dataset, it is uncertain how much the model will learn from processing different data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ef7c0",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "#### General Research:\n",
    "\n",
    "Darya Paspelava. Computer Vision Object Detection: Challenges Faced. Available at: https://www.exposit.com/blog/computer-vision-object-detection-challenges-faced/  (Accessed: 28 July 2023)\n",
    "\n",
    "Liu, Z., Cai, Y., Wang, H. et al. Surrounding Objects Detection and Tracking for Autonomous Driving Using LiDAR and Radar Fusion. Chin. J. Mech. Eng. 34, 117 (2021). https://doi.org/10.1186/s10033-021-00630-y\n",
    "\n",
    "Gaudenz Boesch. Object Detection in 2023: The Definitive Guide. Available at: https://viso.ai/deep-learning/object-detection/ (Accessed: 29 July 2023)\n",
    "\n",
    "Waldschmidt, C., Hasch, J. and Menzel, W., 2021. Automotive radar‚ÄîFrom first efforts to future systems. IEEE Journal of Microwaves, 1(1), pp.135-148.\n",
    "\n",
    "Nobis, F., Geisslinger, M., Weber, M., Betz, J. and Lienkamp, M., 2019, October. A deep learning-based radar and camera sensor fusion architecture for object detection. In 2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF) (pp. 1-7). IEEE.\n",
    "\n",
    "\n",
    "#### Datasets:\n",
    "Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., Beijbom, O., no year. nuScenes: A multimodal dataset for autonomous driving. Available at: https://doi.org/10.48550/arXiv.1903.11027\n",
    "\n",
    "Ouaknine, A., Newson, A., Rebut, J., Tupin, F. and P√©rez, P., 2021. 'Carrada dataset: Camera and automotive radar with range-angle-doppler annotations'. In 2020 25th International Conference on Pattern Recognition (ICPR), pp. 5068-5075. IEEE.\n",
    "\n",
    "Paek, D.H., Kong, S.H. and Wijaya, K.T., 2022. 'K-Radar: 4D Radar Object Detection Dataset and Benchmark for Autonomous Driving in Various Weather Conditions'. arXiv preprint arXiv:2206.08171.\n",
    "\n",
    "Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G. and Beijbom, O., 2020. 'nuscenes: A multimodal dataset for autonomous driving'. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11621-11631.\n",
    "\n",
    "Zhang, A., Nowruzi, F.E. and Laganiere, R., 2021, May. 'RADDet: Range-Azimuth-Doppler based radar object detection for dynamic road users'. In 2021 18th Conference on Robots and Vision (CRV), pp. 95-102. IEEE.\n",
    "\n",
    "Sheeny, M., De Pellegrin, E., Mukherjee, S., Ahrabian, A., Wang, S. and Wallace, A., 2021, May. 'RADIATE: A radar dataset for automotive perception in bad weather'. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 1-7. IEEE.\n",
    "\n",
    "Mostajabi, M., Wang, C.M., Ranjan, D. and Hsyu, G., 2020. 'High-resolution radar dataset for semi-supervised learning of dynamic objects'. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 100-101.\n",
    "\n",
    "Patrick LLGC, 2023. 'Astyx dataset'. Available at: https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/astyx_dataset.html (Accessed: 5 August 2023).\n",
    "\n",
    "CRUW Dataset, 2023. 'CRUW: Radar Dataset for 2D Object Detection'. Available at: https://www.cruwdataset.org/ (Accessed: 5 August 2023).\n",
    "\n",
    "Intelligent Vehicles, 2023. 'View of Delft: A Radar and Vision Dataset for Autonomous Driving in the Rain'. Available at: https://intelligent-vehicles.org/datasets/view-of-delft/ (Accessed: 5 August 2023).\n",
    "\n",
    "\n",
    "#### Models:\n",
    "Redmon, J. and Farhadi, A., 2018. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767.\n",
    "\n",
    "\n",
    "Broedermann, T., Sakaridis, C., Dai, D. and Van Gool, L., 2023. HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection. ETH Zurich/MPI for Informatics/KU Leuven. Available at: https://doi.org/10.48550/arXiv.2206.15157\n",
    "\n",
    "Yan, X., Gao, J., Zheng, C., Zheng, C., Zhang, R., Cui, S. and Li, Z., 2022, October. 2dpass: 2d priors assisted semantic segmentation on lidar point clouds. Cham: Springer Nature Switzerland. Available at: https://arxiv.org/abs/2207.04397\n",
    "\n",
    "SJ√ñBERG, A. (2023) Investigation regarding the performance of yolov8 in pedestrian detection, Investigation regarding the Performance of YOLOv8 in Pedestrian Detection. Available at: https://kth.diva-portal.org/smash/get/diva2:1778368/FULLTEXT01.pdf (Accessed: 05 August 2023).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
