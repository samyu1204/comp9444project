{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c77fe9",
   "metadata": {},
   "source": [
    "# COMP9444 Group Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76861f",
   "metadata": {},
   "source": [
    "## 1. Introduction, Motivation, and/or Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b2962",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Our project aims to leverage computer vision neural network to improve object detection of images during both daytime and nighttime environments. The ability to accurately detect and recognize objects in varying lighting condition has become crucial for the functionalities of many modern day applications; some examples would be autonomous vehicles, surveillance and security systems.\n",
    "\n",
    "Consider the two images below. It is imperative that everything in left image is very easy to identify, and when contrasted to the image on the right it really highlights just how much harder it is to identify objects with low luminosity.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"https://www.exposit.com/wp-content/webp-express/webp-images/doc-root/wp-content/uploads/2021/04/Illumination_conditions_as_a_challenge_of_comp.width-800.jpg.webp\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Motivation\n",
    "Modern day computer vision neural networks often fail to perform well in nighttime object detection (inaccurate detection of objects in low luminosity environments). Nighttime environment factors like shadow, limited luminosity, and visibility makes it challenging for the network to classify objects. With this problem, it can hinder the effectiveness and safety of pre-existing computer vision applications like surveillance, which requires all day monitoring.\n",
    "\n",
    "Researchers have made advancements in enhancing accuracy for low-light detection. An example is the REDI low-light enhancement algorithm, which effectively filters noise in low-light conditions and performs detection on the resulting image.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/lowlight.png\" />\n",
    "</div>\n",
    "\n",
    "Here (a) through to (d) are stages of REDI algorithm filtering. However, there are many downsides to this algorithm like loss of details, over-correction, and high computational cost. This would pose a challenge as it would add extra complexity and computational stress on existing models.\n",
    "\n",
    "Solving day/night object detection will definitely bring significant enhancements in the real world, and some key areas of improvements are autonomous driving, surveillance and security systems. This is not only an exciting technical challenge for researchers, but also has the potential to open up new possibilities for neural network computer vision advancements.\n",
    "\n",
    "### Problem Statements\n",
    "Key challenges that requires to be address by our models are:\n",
    "1. The model requires to handle varying levels of brightness within the image.\n",
    "2. Removing noise from nighttime image, as image taken at night might have more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfb5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e923798",
   "metadata": {},
   "source": [
    "## 2. Exploration Analysis or Data or RL Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f399faf",
   "metadata": {},
   "source": [
    "## 3. Models and/or Methods\n",
    "\n",
    "### 2DPASS \n",
    "Link to paper: https://arxiv.org/pdf/2210.04208.pdf\n",
    "\n",
    "#### Model Introduction\n",
    "This model is an Assisted Semantic Segmentation method that boosts the representation learning on point clouds. A notable advantage of this model is that \n",
    "Advantages of this model is that it does not require strict pair data alignments between the camera and LiDAR data. \n",
    "\n",
    "The 2DPASS method leverages an auxiliary model fusion and multi-scale fusion to single knowledge distillation (MSFSKD) to acquire richer semantic and structural information from the multi-modal data. This is a significant improvement over baseline models where models only use point cloud.\n",
    "\n",
    "\n",
    "### HRFUser\n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/2206.15157.pdf\n",
    "\n",
    "Link to project on gitHub: https://github.com/timbroed/HRFuser\n",
    "\n",
    "### Model Introduction\n",
    "\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "While numerous recent research focus on fusing specific pairs of sensors—such as camera with lidar or radar—by leveraging architectural components relevant to the investigated context, the literature lacks a general and modular sensor fusion architecture. We have HRFuser, a modular architecture for multi-modal 2D object identification. It multiresolutionly integrates numerous sensors and scales to an indefinite number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "HRFuser have a slight special architecture being shown as follow:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-architecture.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "Because of extended layer of input, HRUser results in a better training of combination Data on not just cameras but also multiple type of sensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254fdf1",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 2DPASS Results\n",
    "\n",
    "#### 2DPASS Trained on Mini-Dataset\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/mini.png\" />\n",
    "</div>\n",
    "\n",
    "#### 2DPASS Pretrained Model\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/pretrained.png\" />\n",
    "</div>\n",
    "\n",
    "#### Model Results\n",
    "| Model                | mIoU | Accuracy |\n",
    "|----------------------|------|----------|\n",
    "| 2DPASS (Mini-dataset)| 36%  | 56%      |\n",
    "| 2DPASS (Pretrained)  | 81%  | 63%      |\n",
    "\n",
    "Major improvements in accuracy and mIoU are both significant for the pretrained model which was initially trained on the full dataset. Note, that this result is worse than the one displayed in the paper as their model was trained with additional validation set and using instance-level augmentation.\n",
    "\n",
    "#### Epoch Training Steps\n",
    "NOTE: X-axis is number of epoch.\n",
    "##### mIoU vs Epoch \n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/miou_r.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "#### Best mIoU vs Epoch\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/miou.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "From the mIoU curves and best mIoU curve(smoothened out), we see that around 8000 epoch there are no significant improves in the mIoU value, emphasizing that further training after 8000 epoch does not improve the model, and could lead to overfitting existing data.\n",
    "\n",
    "##### Accuracy vs Epoch \n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/accuracy.png\" width=\"700px\" />\n",
    "</div>\n",
    "The accuracy during the training of the model behaves similarly to the mIoU curve as optimum accuracy is reached around 8000 epoche\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### HRFUser Results\n",
    "\n",
    "#### General sample images results:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser_result.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-Result-2.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Images output from Nuscene MiniDataset after train (note this is only a few out of more than 500 results):\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuser_Result.png\" width=\"1000px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "##### Front Camera:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Camera:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "##### Back Left Camera:\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "##### Back Right Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR_Camera2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR-Camera1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d9427",
   "metadata": {},
   "source": [
    "## 5. Discussion\n",
    "### 2DPASS Discussion\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We have trained the 2DPASS model on a Nvidia 4060 laptop graphics card with 16 gigabytes of RAM. \n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "Training parameters have been pre-tuned by the developers as:\n",
    "- Learning Rate: 0.24\n",
    "- Optimizer: SGD\n",
    "- Momentum: 0.9\n",
    "- Weight Decay: 1.0e-4\n",
    "\n",
    "#### Model Architecture\n",
    "This model significantly improves upon simple image computer vision neural networks, as 2DPASS introduces lidar detection combined with the use of image. This more accurately detects the existence and classification of the object even in low luminosity environments.\n",
    "\n",
    "#### Training Time\n",
    "The training process of our model on the mini-dataset took approximately 5 hours, which is due to our computer’s limited memory as it was only able to manage a batch training size of one. Also, due to the limited variety in the mini-dataset, we observed that the val/mIoU failed to show improvements over the last 50 records, which shows that a lot of the computation towards the end of training did not achieve any notable performance improvements.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/train_time.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "Originally running the model on the whole 80 gigabytes data requires too much computational power and time, so we resorted to using the mini-training set instead, which was much faster to train.\n",
    "\n",
    "Training on a much smaller dataset could potentially introduce overfitting of data and lead to inaccurate results, in this case we have used their pre-trained model to compare results before drawing conclusions.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/overfit.png\" />\n",
    "</div>\n",
    "The above is the result from testing the model trained with the mini-dataset, and here we can clearly see a case of overfitting where all vehicle like objects are recognised as cars explaining the high accuracy in car predictions and basically 0% accuracy in all other vehicles detections.\n",
    "\n",
    "Our main challenges occurred within our limited ability to modify the model, as the training time even on a much smaller dataset took up to five hours. To tackle this problem, we have introduced early-stopping of the training, where if we do not see noticeable improvements on the mIoU(mean intersection over Union) value over five epochs of training we will manually exit the training. However, finding a sweet spot for the improvement was difficult and is hard to optimise. Moreover, as training is also dependent on the distribution of the dataset, it is uncertain how much the model will learn from processing different data.\n",
    "\n",
    "\n",
    "\n",
    "### HRFUser Model Discussion\n",
    "\n",
    "\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We train this data on another machine which is a Linux-Sub-System Machine but does not have GPU onboard, the training has to be on CPU. We have to reduce the batch size and the dataset. The training would took ages for this to happen so we have to make use of pre-trained weighted\n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of Nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "As we have insufficient training resources, for this device working on this model, we have to make use of the pre-trained weight provided by the Research Paper\n",
    "\n",
    "#### Model Architecture\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "#### Training Time\n",
    "It would take ages, approximately more than 8 hours on the device working on this. But to use pre-trained weight, it would cost us approximately an hour to perform.\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/train_time_HR_Fuser.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "The challenge for this model is that it makes use of the mmdet library and mmcv but the running environment on the paper provided is only suitable with a Linux running environment. We would have to create a WSL ( Window Subsystem for Linux) and then run the project on it. Also, WSL cannot connect and refer directly with the Window's CUDA and GPU, we have to modify the code for it to accept CPU train on Pytorch. As CPU train is very limited, we have to fully reduced the batch size to 1 and then perform training on a small MiniDataset instead of a huge one. Also, making use of a pre-trained weight would save us much time rather than training the whole process.\n",
    "\n",
    "Overall HRuser actually better the performance on low-light detection since it was able to fuse all the data input such as camera and sensors together for the train. Thus, improve significantly the detection on varies foggy and low-light environment.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR_Fuser_discussion.png\" />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac898b0",
   "metadata": {},
   "source": [
    "## HRFuser code demo on a pre trained weight with output file generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce02e562",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\compat.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmcv\u001b[39;00m \u001b[39mimport\u001b[39;00m Config, DictAction\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmcv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunner\u001b[39;00m \u001b[39mimport\u001b[39;00m get_dist_info, init_dist\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmcv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_git_hash\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmdet\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mmcv\\runner\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase_module\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseModule, ModuleList, Sequential\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase_runner\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseRunner\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m RUNNERS, build_runner\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m (CheckpointLoader, _load_checkpoint,\n\u001b[0;32m      6\u001b[0m                          _load_checkpoint_with_prefix, load_checkpoint,\n\u001b[0;32m      7\u001b[0m                          load_state_dict, save_checkpoint, weights_to_cpu)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mmcv\\runner\\base_runner.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmmcv\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m is_module_wrapper\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m load_checkpoint\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdist_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_dist_info\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m \u001b[39mimport\u001b[39;00m HOOKS, Hook\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mmcv\\runner\\checkpoint.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtempfile\u001b[39;00m \u001b[39mimport\u001b[39;00m TemporaryDirectory\n\u001b[0;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m Optimizer\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmmcv\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optical_flow\u001b[39;00m \u001b[39mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stereo_matching\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     CarlaStereo,\n\u001b[0;32m      4\u001b[0m     CREStereo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     SintelStereo,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcaltech\u001b[39;00m \u001b[39mimport\u001b[39;00m Caltech101, Caltech256\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\_optical_flow.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_png_16\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_pfm, verify_str_arg\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvision\u001b[39;00m \u001b[39mimport\u001b[39;00m VisionDataset\n\u001b[0;32m     17\u001b[0m T1 \u001b[39m=\u001b[39m Tuple[Image\u001b[39m.\u001b[39mImage, Image\u001b[39m.\u001b[39mImage, Optional[np\u001b[39m.\u001b[39mndarray], Optional[np\u001b[39m.\u001b[39mndarray]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m \u001b[39mimport\u001b[39;00m urlparse\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_zoo\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\__init__.py:45\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib3\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[0;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m charset_normalizer_version\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\exceptions.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mrequests.exceptions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[39mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39murllib3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m BaseHTTPError\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m CompatJSONDecodeError\n\u001b[0;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRequestException\u001b[39;00m(\u001b[39mIOError\u001b[39;00m):\n\u001b[0;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m    request.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\compat.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# -------\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Pythons\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# -------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[39m# Syntax sugar.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\charset_normalizer\\__init__.py:24\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m from_bytes, from_fp, from_path\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m detect\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m CharsetMatch, CharsetMatches\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\charset_normalizer\\api.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mos\u001b[39;00m \u001b[39mimport\u001b[39;00m PathLike\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, BinaryIO, List, Optional, Set\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcd\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     coherence_ratio,\n\u001b[0;32m      7\u001b[0m     encoding_languages,\n\u001b[0;32m      8\u001b[0m     mb_encoding_languages,\n\u001b[0;32m      9\u001b[0m     merge_coherence_ratios,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m IANA_SUPPORTED, TOO_BIG_SEQUENCE, TOO_SMALL_SEQUENCE, TRACE\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmd\u001b[39;00m \u001b[39mimport\u001b[39;00m mess_ratio\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\charset_normalizer\\cd.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter \u001b[39mas\u001b[39;00m TypeCounter, Dict, List, Optional, Tuple\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39massets\u001b[39;00m \u001b[39mimport\u001b[39;00m FREQUENCIES\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m KO_NAMES, LANGUAGE_SUPPORTED_COUNT, TOO_SMALL_SEQUENCE, ZH_NAMES\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmd\u001b[39;00m \u001b[39mimport\u001b[39;00m is_suspiciously_successive_range\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m CoherenceMatches\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.utils import get_git_hash\n",
    "\n",
    "from mmdet import __version__\n",
    "from mmdet.apis import init_random_seed, set_random_seed, train_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.utils import collect_env, get_root_logger\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a527dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace()\n",
    "\n",
    "args.config = \"./HRFuser_config/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_bn.py\"\n",
    "args.work_dir = \"demo/output\"  # Update with your value if required\n",
    "args.resume_from = \"./checkpoints/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_latest.pth\"\n",
    "args.no_validate = False  # Update with your value if required\n",
    "args.gpus = None  # Update with your value if required\n",
    "args.gpu_ids = None  # Update with your value if required\n",
    "args.seed = None  # Update with your value if required\n",
    "args.deterministic = False  # Update with your value if required\n",
    "args.options = None  # Update with your value if required\n",
    "args.cfg_options = {\"data.test.samples_per_gpu\": 1}\n",
    "args.launcher = \"none\"  # Update with your value if required\n",
    "args.local_rank = 0  # Update with your value if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9284d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'LOCAL_RANK' not in os.environ:\n",
    "    os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "\n",
    "if args.options and args.cfg_options:\n",
    "    raise ValueError(\n",
    "        '--options and --cfg-options cannot be both '\n",
    "        'specified, --options is deprecated in favor of --cfg-options')\n",
    "if args.options:\n",
    "    warnings.warn('--options is deprecated in favor of --cfg-options')\n",
    "    args.cfg_options = args.options\n",
    "\n",
    "cfg = Config.fromfile(args.config)\n",
    "if args.cfg_options is not None:\n",
    "    cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if args.work_dir is not None:\n",
    "    cfg.work_dir = args.work_dir\n",
    "elif cfg.get('work_dir', None) is None:\n",
    "    cfg.work_dir = osp.join('./work_dirs',\n",
    "                            osp.splitext(osp.basename(args.config))[0])\n",
    "if args.resume_from is not None:\n",
    "    cfg.resume_from = args.resume_from\n",
    "if args.gpu_ids is not None:\n",
    "    cfg.gpu_ids = args.gpu_ids\n",
    "else:\n",
    "    cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)\n",
    "\n",
    "if args.launcher == 'none':\n",
    "    distributed = False\n",
    "else:\n",
    "    distributed = True\n",
    "    init_dist(args.launcher, **cfg.dist_params)\n",
    "    _, world_size = get_dist_info()\n",
    "    cfg.gpu_ids = range(world_size)\n",
    "\n",
    "os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "\n",
    "cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
    "logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
    "\n",
    "meta = dict()\n",
    "\n",
    "env_info_dict = collect_env()\n",
    "env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n",
    "dash_line = '-' * 60 + '\\n'\n",
    "logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "            dash_line)\n",
    "meta['env_info'] = env_info\n",
    "meta['config'] = cfg.pretty_text\n",
    "\n",
    "logger.info(f'Distributed training: {distributed}')\n",
    "logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "if 'seed' in cfg.keys() and cfg.seed is not None:\n",
    "    seed = cfg.seed\n",
    "else:\n",
    "    seed = init_random_seed(args.seed)\n",
    "logger.info(f'Set random seed to {seed}, '\n",
    "            f'deterministic: {args.deterministic}')\n",
    "set_random_seed(seed, deterministic=args.deterministic)\n",
    "cfg.seed = seed\n",
    "meta['seed'] = seed\n",
    "meta['exp_name'] = osp.basename(args.config)\n",
    "\n",
    "model = build_detector(\n",
    "    cfg.model,\n",
    "    train_cfg=cfg.get('train_cfg'),\n",
    "    test_cfg=cfg.get('test_cfg'))\n",
    "model.init_weights()\n",
    "\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "if len(cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(cfg.data.val)\n",
    "    val_dataset.pipeline = cfg.data.train.pipeline\n",
    "    datasets.append(build_dataset(val_dataset))\n",
    "if cfg.checkpoint_config is not None:\n",
    "    cfg.checkpoint_config.meta = dict(\n",
    "        mmdet_version=__version__ + get_git_hash()[:7],\n",
    "        CLASSES=datasets[0].CLASSES)\n",
    "\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "train_detector(\n",
    "    model,\n",
    "    datasets,\n",
    "    cfg,\n",
    "    distributed=distributed,\n",
    "    validate=(not args.no_validate),\n",
    "    timestamp=timestamp,\n",
    "    meta=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ef7c0",
   "metadata": {},
   "source": [
    "### References (To be cleaned up later)\n",
    "https://www.exposit.com/blog/computer-vision-object-detection-challenges-faced/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
