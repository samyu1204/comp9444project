{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c77fe9",
   "metadata": {},
   "source": [
    "# COMP9444 Group Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76861f",
   "metadata": {},
   "source": [
    "# 1. Introduction, Motivation, and/or Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b2962",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Our project aims to leverage computer vision neural network to improve object detection of images during both daytime and nighttime environments. The ability to accurately detect and recognize objects in varying lighting condition has become crucial for the functionalities of many modern day applications; some examples would be autonomous vehicles, surveillance and security systems.\n",
    "\n",
    "Consider the two images below. It is imperative that everything in left image is very easy to identify, and when contrasted to the image on the right it really highlights just how much harder it is to identify objects with low luminosity.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"https://www.exposit.com/wp-content/webp-express/webp-images/doc-root/wp-content/uploads/2021/04/Illumination_conditions_as_a_challenge_of_comp.width-800.jpg.webp\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Motivation\n",
    "Modern day computer vision neural networks often fail to perform well in nighttime object detection (inaccurate detection of objects in low luminosity environments). Nighttime environment factors like shadow, limited luminosity, and visibility makes it challenging for the network to classify objects. With this problem, it can hinder the effectiveness and safety of pre-existing computer vision applications like surveillance, which requires all day monitoring.\n",
    "\n",
    "Researchers have made advancements in enhancing accuracy for low-light detection. An example is the REDI low-light enhancement algorithm, which effectively filters noise in low-light conditions and performs detection on the resulting image.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/lowlight.png\" />\n",
    "</div>\n",
    "\n",
    "Here (a) through to (d) are stages of REDI algorithm filtering. However, there are many downsides to this algorithm like loss of details, over-correction, and high computational cost. This would pose a challenge as it would add extra complexity and computational stress on existing models.\n",
    "\n",
    "Solving day/night object detection will definitely bring significant enhancements in the real world, and some key areas of improvements are autonomous driving, surveillance and security systems. This is not only an exciting technical challenge for researchers, but also has the potential to open up new possibilities for neural network computer vision advancements.\n",
    "\n",
    "### Problem Statements\n",
    "Key challenges that requires to be address by our models are:\n",
    "1. The model requires to handle varying levels of brightness within the image.\n",
    "2. Removing noise from nighttime image, as image taken at night might have more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e923798",
   "metadata": {},
   "source": [
    "# 2. Exploration Analysis or Data or RL Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f399faf",
   "metadata": {},
   "source": [
    "# 3. Models and/or Methods\n",
    "## YOLOv8s\n",
    "Link to document: https://docs.ultralytics.com/\n",
    "\n",
    "#### Model Introduction\n",
    "\n",
    "The state-of-the-art object detection system YOLO (You Only Look Once) is a single-stage detector.Since the initial release of YOLOv1 in 2016, the YOLO family has been refined and upgraded to YOLOv8. We chose YOLOv8, the most recent iteration of the YOLO series, to conduct 2D object detection for this project. Instead of a single model, YOLOv8 has created multiple versions, each with its own distinctive characteristics. Our project uses YOLOv8s. These version are:\n",
    "1. YOLOv8n ---- The nano version\n",
    "2. YOLOv8s ---- The small version\n",
    "3. YOLOv8m ---- The medium version\n",
    "4. YOLOv8l ---- The large version\n",
    "The performance of recent version of YOLO is shown below:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8 performance.png\" />\n",
    "</div>\n",
    "refer to https://github.com/ultralytics/ultralytics\n",
    "\n",
    "\n",
    "Architecture of YOLOv8 is shown below:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8 network.jpeg\" />\n",
    "</div>\n",
    "refer to https://arxiv.org/pdf/2304.00501.pdf\n",
    "\n",
    "\n",
    "\n",
    "Compared with previous YOLO algorithms, YOLOv8 has a new backnone network, a new anchor-free detection head and a new loss function. It uses a similar backbone network as YOLOv5 but changes CSPLayer to C2f which combines high-level features with contextual information to improve detection accuracy. The anchor-free detection head allows each branch to focus on its own tasks and improve overall accuracy. The new loss function uses CIoU and DFL loss function for bounding box loss and binary cross-entropy for classification loss, which helps to improve detection performance , especially when detecting small objects.\n",
    "\n",
    "## HRFUser\n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/2206.15157.pdf\n",
    "\n",
    "\n",
    "\n",
    "#### Model Introduction\n",
    "\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "While numerous recent research focus on fusing specific pairs of sensors—such as camera with lidar or radar—by leveraging architectural components relevant to the investigated context, the literature lacks a general and modular sensor fusion architecture. We have HRFuser, a modular architecture for multi-modal 2D object identification. It multiresolutionly integrates numerous sensors and scales to an indefinite number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "HRFuser have a slight special architecture being shown as follow:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-architecture.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "Because of extended layer of input, HRUser results in a better training of combination Data on not just cameras but also multiple type of sensors\n",
    "\n",
    "## 2DPASS \n",
    "Link to paper: https://arxiv.org/pdf/2210.04208.pdf\n",
    "\n",
    "#### Model Introduction\n",
    "The 2DPASS (2D Priors Assisted Semantic Segmentation) model has a fusion architecture to boost representation learning on point clouds, requiring both 2D and 3D inputs from camera images and LiDAR point clouds.\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/2DPASS_camera_lidar.png\" />\n",
    "</div>\n",
    "\n",
    "LiDAR offers accurate depth information regardless of lighting conditions so it is not inhibited by dark conditions and will be able to effectively assist in day/night object detection. However, the data that LiDAR captures is sparse and textureless so object detection models based on LiDAR point clouds alone may not perform as well. Cameras images are able to provide dense colour information and fine grained textures but provide ambiguous depth perception and can be unclear in low light conditions such as at night. As cameras and LiDARs have differing strengths and weaknesses, they complement each other so 2DPASS was chosen as it uses both input modals and we check if this provides better night time detection compared to models only using 2D images.\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/2DPASS_model_structure.png\" />\n",
    "</div>\n",
    "\n",
    "The model uses 2 independent networks which run in parallel to extract multi-scale features from the 2D images and 3D point clouds. The 2D network samples small patches from the full camera image (480 x 320) and applies a ResNet34 encoder with 2D convolutions.\n",
    " \n",
    "Multi scale fusion to single knowledge distillation (MSFSKD) then enhances the 3D network by utilising multi-modal features such as texture and colour from the 2D images and applying this to the 3D network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612e519",
   "metadata": {},
   "source": [
    "# 4. Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254fdf1",
   "metadata": {},
   "source": [
    "### YOLOv8s Result\n",
    "#### Model Result\n",
    "According to the table below, YOLOv8s performs better in daytime detection than nighttime detection. For nighttime detection, the mAP is only 18%.\n",
    "\n",
    "| Dataset     | mAP (%)     |\n",
    "|-------------|-------------|\n",
    "|overall      | 40%         |\n",
    "|daytime      | 65%         |\n",
    "|nighttime    | 18%         |\n",
    "\n",
    "Based on the confusion matrix of model on overall dataset shown below, we can see that car has the highest precision, while other categories are easily misdetected as background.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_confusion_matrix.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "#### Prediction Output\n",
    "\n",
    "The comparison between prediction and label shows that although YOLOv8s performs better than YOLOv3, the result is still not ideal due to misdetection.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_prediction.jpeg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_label.jpeg\" width=\"700px\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa4c2a",
   "metadata": {},
   "source": [
    "#### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acbee3",
   "metadata": {},
   "source": [
    "### HRFUser Results\n",
    "\n",
    "#### General sample images results:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser_result.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-Result-2.png\" width=\"450px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Images output from Nuscene MiniDataset after train (note this is only a few out of more than 500 results):\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuser_Result.png\" width=\"750px\" />\n",
    "</div>\n",
    "\n",
    "##### Front Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC1.jpg\" width=\"375px\" />\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC2.jpg\" width=\"375px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC1.jpg\" width=\"375px\" />\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC2.jpg\" width=\"375px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Left Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC1.jpg\" width=\"375px\" />\n",
    "    <img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC2.jpg\" width=\"375px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Right Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px; display: flex;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR_Camera2.jpg\" width=\"375px\" />\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR-Camera1.jpg\" width=\"375px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31200dc",
   "metadata": {},
   "source": [
    "Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2e483",
   "metadata": {},
   "source": [
    "### 2DPASS Results\n",
    "\n",
    "#### 2DPASS Trained on Mini-Dataset\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/mini.png\" width=\"850px\" />\n",
    "</div>\n",
    "\n",
    "#### 2DPASS Pretrained Model\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/pretrained.png\" width=\"850px\" />\n",
    "</div>\n",
    "\n",
    "#### Model Results (organised into a table)\n",
    "**Per Class IoU**\n",
    "\n",
    "| Class                         | IoU Our Training (%) | IoU Pretrained (%) |\n",
    "|-------------------------------|----------|----------|\n",
    "| Movable Object Barrier        |  0.00    |  84.60   |\n",
    "| Bicycle               |  0.00    |  64.95   |\n",
    "| Bus             |  23.21   |  92.30   |\n",
    "| Car                   |  84.27   |  90.76   |\n",
    "| Construction          |   NaN    |   NaN    |\n",
    "| Motorcycle            |  0.28    |  71.84   |\n",
    "| Human Pedestrian Adult        |  44.91   |  86.12   |\n",
    "| Movable Object Trafficcone    |  0.00    |  47.46   |\n",
    "| Trailer               |  0.00    |  86.06   |\n",
    "| Truck                 |  66.60   |  71.99   |\n",
    "| Driveable Surface        |  89.26   |  96.10   |\n",
    "| Other                    |  1.19    |  85.00   |\n",
    "| Sidewalk                 |  33.79   |  72.88   |\n",
    "| Terrain                  |  59.13   |  86.08   |\n",
    "| Manmade                |  72.33   |  91.41   |\n",
    "| Vegetation             |  69.03   |  91.09   |\n",
    "\n",
    "**Global Metrics**\n",
    "\n",
    "| Metric       | Our Training           | Pretrained              |\n",
    "|--------------|------------------|---------------------|\n",
    "| Accuracy      | 0.56             | 0.63                |\n",
    "| mIoU     | 0.36             | 0.81                |\n",
    "\n",
    "Major improvements in accuracy and mIoU are both significant for the pretrained model which was initially trained on the full dataset. Note, that this result is worse than the one displayed in the paper as their model was trained with additional validation set and using instance-level augmentation.\n",
    "\n",
    "#### Epoch Training Steps\n",
    "NOTE: X-axis is number of epoch.\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center; margin-bottom: 30px;\">\n",
    "  <div style=\"text-align: center; margin-right: 5px;\">\n",
    "    <h4>mIoU vs Epoch</h4>\n",
    "    <img src=\"./images/miou_r.png\" alt=\"mIoU vs Epoch\" width=\"500px\" style=\"margin-bottom: 20px;\" />\n",
    "  </div>\n",
    "  <div style=\"text-align: center; margin-left: 5px;\">\n",
    "    <h4>Best mIoU vs Epoch</h4>\n",
    "    <img src=\"./images/miou.png\" alt=\"Best mIoU vs Epoch\" width=\"500px\" style=\"margin-bottom: 20px;\" />\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "From the mIoU curves and best mIoU curve(smoothened out), we see that around 8000 epoch there are no significant improves in the mIoU value, emphasizing that further training after 8000 epoch does not improve the model, and could lead to overfitting existing data.\n",
    "\n",
    "##### Accuracy vs Epoch \n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/accuracy.png\" width=\"500px\" />\n",
    "</div>\n",
    "The accuracy during the training of the model behaves similarly to the mIoU curve as optimum accuracy is reached around 8000 epoche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6f9ef",
   "metadata": {},
   "source": [
    "Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6353506",
   "metadata": {},
   "source": [
    "# Model Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed47a46",
   "metadata": {},
   "source": [
    "## Yolov3 Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce314dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "au9g0521",
   "metadata": {},
   "source": [
    "## Yolov8 Code\n",
    "### Experiment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "as580627",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/dataset\n",
    "%cd /content/dataset\n",
    "!pip install roboflow\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key='U9qUuMcwNCEJrQCd2sUC')\n",
    "project = rf.workspace('unsw-kgzbp').project('comp9444-v2')\n",
    "dataset = project.version(1).download('yolov8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agj02856",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics==8.0.20\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "import ultralytics\n",
    "import time\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca58aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=150 imgsz=1280 plots=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kl062697",
   "metadata": {},
   "source": [
    "The result is shown below\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8 result.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ac7b2",
   "metadata": {},
   "source": [
    "## 2DPASS Code with Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631464cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samyu\\Code\\comp9444project\\2DPASS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/samyu/Code/comp9444project/2DPASS\")\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import datetime\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from easydict import EasyDict\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.profiler import SimpleProfiler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, StochasticWeightAveraging\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from dataloader.dataset import get_model_class, get_collate_class\n",
    "from dataloader.pc_dataset import get_pc_model_class\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from torch import distributed as dist\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20734da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 1, 'model_params': {'model_architecture': 'arch_2dpass', 'input_dims': 4, 'spatial_shape': [1000, 1000, 70], 'scale_list': [2, 4, 8, 16, 16, 16], 'hiden_size': 256, 'num_classes': 17, 'backbone_2d': 'resnet34', 'pretrained2d': False}, 'dataset_params': {'training_size': 28130, 'dataset_type': 'point_image_dataset_nus', 'pc_dataset_type': 'nuScenes', 'collate_type': 'collate_fn_default', 'ignore_label': 0, 'label_mapping': './config/label_mapping/nuscenes.yaml', 'resize': [400, 240], 'color_jitter': [0.4, 0.4, 0.4], 'flip2d': 0.5, 'image_normalizer': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'max_volume_space': [50, 50, 3], 'min_volume_space': [-50, -50, -4], 'train_data_loader': {'data_path': './dataset/nuscenes/', 'batch_size': 1, 'shuffle': True, 'num_workers': 8, 'rotate_aug': True, 'flip_aug': True, 'scale_aug': True, 'transform_aug': True, 'dropout_aug': True}, 'val_data_loader': {'data_path': './dataset/nuscenes', 'shuffle': False, 'num_workers': 8, 'batch_size': 1, 'rotate_aug': False, 'flip_aug': False, 'scale_aug': False, 'transform_aug': False, 'dropout_aug': False}}, 'train_params': {'max_num_epochs': 80, 'learning_rate': 0.24, 'optimizer': 'SGD', 'lr_scheduler': 'CosineAnnealingLR', 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001, 'lambda_seg2d': 1, 'lambda_xm': 0.05, 'lambda_lovasz': 1}, 'gpu': [0], 'seed': 0, 'config_path': 'config/2DPASS-nuscenese.yaml', 'log_dir': 'default', 'monitor': 'val/mIoU', 'stop_patience': 50, 'save_top_k': 1, 'check_val_every_n_epoch': 1, 'SWA': False, 'baseline_only': False, 'test': True, 'fine_tune': False, 'pretrain2d': False, 'num_vote': 1, 'submit_to_server': False, 'checkpoint': None, 'debug': False}\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "34149 lidarseg,\n",
      "Done loading in 1.859 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Total 162 scenes in the val split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "def load_yaml(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        try:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except:\n",
    "            config = yaml.load(f)\n",
    "    return config\n",
    "\n",
    "def parse_config():\n",
    "    config = load_yaml('config/2DPASS-nuscenese.yaml')  # Load config from yaml file\n",
    "\n",
    "    # manually set the values that were previously command-line arguments\n",
    "    config['gpu'] = [0]\n",
    "    config['seed'] = 0\n",
    "    config['config_path'] = 'config/2DPASS-nuscenese.yaml'\n",
    "    config['log_dir'] = 'default'\n",
    "    config['monitor'] = 'val/mIoU'\n",
    "    config['stop_patience'] = 50\n",
    "    config['save_top_k'] = 1\n",
    "    config['check_val_every_n_epoch'] = 1\n",
    "    config['SWA'] = False\n",
    "    config['baseline_only'] = False\n",
    "    config['test'] = True\n",
    "    config['fine_tune'] = False\n",
    "    config['pretrain2d'] = False\n",
    "    config['num_vote'] = 1\n",
    "    config['submit_to_server'] = False\n",
    "    config['checkpoint'] = None\n",
    "    config['debug'] = False\n",
    "\n",
    "    return EasyDict(config)\n",
    "\n",
    "\n",
    "def build_loader(config):\n",
    "    pc_dataset = get_pc_model_class(config['dataset_params']['pc_dataset_type'])\n",
    "    dataset_type = get_model_class(config['dataset_params']['dataset_type'])\n",
    "    train_config = config['dataset_params']['train_data_loader']\n",
    "    val_config = config['dataset_params']['val_data_loader']\n",
    "    train_dataset_loader, val_dataset_loader, test_dataset_loader = None, None, None\n",
    "\n",
    "    if not config['test']:\n",
    "        train_pt_dataset = pc_dataset(config, data_path=train_config['data_path'], imageset='train')\n",
    "        val_pt_dataset = pc_dataset(config, data_path=val_config['data_path'], imageset='val')\n",
    "        train_dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_type(train_pt_dataset, config, train_config),\n",
    "            batch_size=train_config[\"batch_size\"],\n",
    "            collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "            shuffle=train_config[\"shuffle\"],\n",
    "            num_workers=train_config[\"num_workers\"],\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        # config['dataset_params']['training_size'] = len(train_dataset_loader) * len(configs.gpu)\n",
    "        val_dataset_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_type(val_pt_dataset, config, val_config, num_vote=1),\n",
    "            batch_size=val_config[\"batch_size\"],\n",
    "            collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "            shuffle=val_config[\"shuffle\"],\n",
    "            pin_memory=True,\n",
    "            num_workers=val_config[\"num_workers\"]\n",
    "        )\n",
    "    else:\n",
    "        if config['submit_to_server']:\n",
    "            test_pt_dataset = pc_dataset(config, data_path=val_config['data_path'], imageset='test', num_vote=val_config[\"batch_size\"])\n",
    "            test_dataset_loader = torch.utils.data.DataLoader(\n",
    "                dataset=dataset_type(test_pt_dataset, config, val_config, num_vote=val_config[\"batch_size\"]),\n",
    "                batch_size=val_config[\"batch_size\"],\n",
    "                collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "                shuffle=val_config[\"shuffle\"],\n",
    "                num_workers=val_config[\"num_workers\"]\n",
    "            )\n",
    "        else:\n",
    "            val_pt_dataset = pc_dataset(config, data_path=val_config['data_path'], imageset='val', num_vote=val_config[\"batch_size\"])\n",
    "            val_dataset_loader = torch.utils.data.DataLoader(\n",
    "                dataset=dataset_type(val_pt_dataset, config, val_config, num_vote=val_config[\"batch_size\"]),\n",
    "                batch_size=val_config[\"batch_size\"],\n",
    "                collate_fn=get_collate_class(config['dataset_params']['collate_type']),\n",
    "                shuffle=val_config[\"shuffle\"],\n",
    "                num_workers=val_config[\"num_workers\"]\n",
    "            )\n",
    "\n",
    "    return train_dataset_loader, val_dataset_loader, test_dataset_loader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parameters\n",
    "    configs = parse_config()\n",
    "    print(configs)\n",
    "\n",
    "    # setting\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, configs.gpu))\n",
    "    num_gpu = len(configs.gpu)\n",
    "\n",
    "    # output path\n",
    "    log_folder = 'logs/' + configs['dataset_params']['pc_dataset_type']\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(log_folder, name=configs.log_dir, default_hp_metric=False)\n",
    "    os.makedirs(f'{log_folder}/{configs.log_dir}', exist_ok=True)\n",
    "    profiler = SimpleProfiler(output_filename=f'{log_folder}/{configs.log_dir}/profiler.txt')\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    # save the backup files\n",
    "    backup_dir = os.path.join(log_folder, configs.log_dir, 'backup_files_%s' % str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')))\n",
    "    if not configs['test']:\n",
    "        os.makedirs(backup_dir, exist_ok=True)\n",
    "        os.system('copy main.py {}'.format(backup_dir))\n",
    "        os.system('copy dataloader/dataset.py {}'.format(backup_dir))\n",
    "        os.system('copy dataloader/pc_dataset.py {}'.format(backup_dir))\n",
    "        os.system('copy {} {}'.format(configs.config_path, backup_dir))\n",
    "        os.system('copy network/base_model.py {}'.format(backup_dir))\n",
    "        os.system('copy network/baseline.py {}'.format(backup_dir))\n",
    "        os.system('copy {}.py {}'.format('network/' + configs['model_params']['model_architecture'], backup_dir))\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(configs.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    np.random.seed(configs.seed)\n",
    "    config_path = configs.config_path\n",
    "\n",
    "    train_dataset_loader, val_dataset_loader, test_dataset_loader = build_loader(configs)\n",
    "    model_file = importlib.import_module('network.' + configs['model_params']['model_architecture'])\n",
    "    my_model = model_file.get_model(configs)\n",
    "\n",
    "    pl.seed_everything(configs.seed)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=configs.monitor,\n",
    "        mode='max',\n",
    "        save_last=True,\n",
    "        save_top_k=configs.save_top_k)\n",
    "\n",
    "    if configs.checkpoint is not None:\n",
    "        print('load pre-trained model...')\n",
    "        if configs.fine_tune or configs.test or configs.pretrain2d:\n",
    "            my_model = my_model.load_from_checkpoint(configs.checkpoint, config=configs, strict=(not configs.pretrain2d))\n",
    "        else:\n",
    "            # continue last training\n",
    "            my_model = my_model.load_from_checkpoint(configs.checkpoint)\n",
    "\n",
    "    if configs.SWA:\n",
    "        swa = [StochasticWeightAveraging(swa_epoch_start=configs.train_params.swa_epoch_start, annealing_epochs=1)]\n",
    "    else:\n",
    "        swa = []\n",
    "\n",
    "    print('Start testing...')\n",
    "    assert num_gpu == 1, 'only support single GPU testing!'\n",
    "    trainer = pl.Trainer(gpus=[i for i in range(num_gpu)],\n",
    "                         accelerator='ddp_spawn',\n",
    "                         resume_from_checkpoint=configs.checkpoint,\n",
    "                         logger=tb_logger,\n",
    "                         profiler=profiler)\n",
    "\n",
    "    trainer.test(my_model, test_dataset_loader if configs.submit_to_server else val_dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85e4f0",
   "metadata": {},
   "source": [
    "Upon running the above code, the training would actually be executed in the temrinal, and I have attached the respective photos below:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/2DPASS Jup_training.png\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/pretrained jup.png\" />\n",
    "</div>\n",
    "\n",
    "This result was analysed in the results section above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f6f04",
   "metadata": {},
   "source": [
    "## HRFuser code demo on a pre trained weight with output file generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce02e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.utils import get_git_hash\n",
    "\n",
    "from mmdet import __version__\n",
    "from mmdet.apis import init_random_seed, set_random_seed, train_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.utils import collect_env, get_root_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a527dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "arguments = ['./HRFuser_config/hrfuser/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_bn.py', # batch-norm\n",
    "             './checkpoints/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_latest.pth',\n",
    "             '--cfg-options', 'data.test.samples_per_gpu=1',\n",
    "             '--show-dir', 'demo/output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9284d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a detector')\n",
    "    parser.add_argument('config', help='train config file path')\n",
    "    parser.add_argument('--work-dir', help='the dir to save logs and models')\n",
    "    parser.add_argument(\n",
    "        '--resume-from', help='the checkpoint file to resume from')\n",
    "    parser.add_argument(\n",
    "        '--no-validate',\n",
    "        action='store_true',\n",
    "        help='whether not to evaluate the checkpoint during training')\n",
    "    group_gpus = parser.add_mutually_exclusive_group()\n",
    "    group_gpus.add_argument(\n",
    "        '--gpus',\n",
    "        type=int,\n",
    "        help='number of gpus to use '\n",
    "        '(only applicable to non-distributed training)')\n",
    "    group_gpus.add_argument(\n",
    "        '--gpu-ids',\n",
    "        type=int,\n",
    "        nargs='+',\n",
    "        help='ids of gpus to use '\n",
    "        '(only applicable to non-distributed training)')\n",
    "    parser.add_argument('--seed', type=int, default=None, help='random seed')\n",
    "    parser.add_argument(\n",
    "        '--deterministic',\n",
    "        action='store_true',\n",
    "        help='whether to set deterministic options for CUDNN backend.')\n",
    "    parser.add_argument(\n",
    "        '--options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file (deprecate), '\n",
    "        'change to --cfg-options instead.')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file. If the value to '\n",
    "        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n",
    "        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n",
    "        'Note that the quotation marks are necessary and that no white space '\n",
    "        'is allowed.')\n",
    "    parser.add_argument(\n",
    "        '--launcher',\n",
    "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
    "        default='none',\n",
    "        help='job launcher')\n",
    "    parser.add_argument('--local_rank', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args(arguments)\n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "\n",
    "    if args.options and args.cfg_options:\n",
    "        raise ValueError(\n",
    "            '--options and --cfg-options cannot be both '\n",
    "            'specified, --options is deprecated in favor of --cfg-options')\n",
    "    if args.options:\n",
    "        warnings.warn('--options is deprecated in favor of --cfg-options')\n",
    "        args.cfg_options = args.options\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc47b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "    # set cudnn_benchmark\n",
    "    if cfg.get('cudnn_benchmark', False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # work_dir is determined in this priority: CLI > segment in file > filename\n",
    "    if args.work_dir is not None:\n",
    "        # update configs according to CLI args if args.work_dir is not None\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        # use config filename as default work_dir if cfg.work_dir is None\n",
    "        cfg.work_dir = osp.join('./work_dirs',\n",
    "                                osp.splitext(osp.basename(args.config))[0])\n",
    "    if args.resume_from is not None:\n",
    "        cfg.resume_from = args.resume_from\n",
    "    if args.gpu_ids is not None:\n",
    "        cfg.gpu_ids = args.gpu_ids\n",
    "    else:\n",
    "        cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    if args.launcher == 'none':\n",
    "        distributed = False\n",
    "    else:\n",
    "        distributed = True\n",
    "        init_dist(args.launcher, **cfg.dist_params)\n",
    "        # re-set gpu_ids with distributed training mode\n",
    "        _, world_size = get_dist_info()\n",
    "        cfg.gpu_ids = range(world_size)\n",
    "\n",
    "    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "    # create work_dir\n",
    "    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "    # dump config\n",
    "    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n",
    "    # init the logger before other steps\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
    "    logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
    "\n",
    "    # init the meta dict to record some important information such as\n",
    "    # environment info and seed, which will be logged\n",
    "    meta = dict()\n",
    "    # log env info\n",
    "    env_info_dict = collect_env()\n",
    "    env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n",
    "    dash_line = '-' * 60 + '\\n'\n",
    "    logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "                dash_line)\n",
    "    meta['env_info'] = env_info\n",
    "    meta['config'] = cfg.pretty_text\n",
    "    # log some basic info\n",
    "    logger.info(f'Distributed training: {distributed}')\n",
    "    logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "    # set random seeds\n",
    "    if 'seed' in cfg.keys() and cfg.seed is not None:\n",
    "        seed = cfg.seed\n",
    "    else:\n",
    "        seed = init_random_seed(args.seed)\n",
    "    logger.info(f'Set random seed to {seed}, '\n",
    "                f'deterministic: {args.deterministic}')\n",
    "    #set_random_seed(seed, deterministic=args.deterministic)\n",
    "    cfg.seed = seed\n",
    "    meta['seed'] = seed\n",
    "    meta['exp_name'] = osp.basename(args.config)\n",
    "\n",
    "    model = build_detector(\n",
    "        cfg.model,\n",
    "        train_cfg=cfg.get('train_cfg'),\n",
    "        test_cfg=cfg.get('test_cfg'))\n",
    "    model.init_weights()\n",
    "\n",
    "    datasets = [build_dataset(cfg.data.train)]\n",
    "    if len(cfg.workflow) == 2:\n",
    "        val_dataset = copy.deepcopy(cfg.data.val)\n",
    "        val_dataset.pipeline = cfg.data.train.pipeline\n",
    "        datasets.append(build_dataset(val_dataset))\n",
    "    if cfg.checkpoint_config is not None:\n",
    "        # save mmdet version, config file content and class names in\n",
    "        # checkpoints as meta data\n",
    "        cfg.checkpoint_config.meta = dict(\n",
    "            mmdet_version=__version__ + get_git_hash()[:7],\n",
    "            config=cfg.pretty_text,\n",
    "            CLASSES=datasets[0].CLASSES)\n",
    "\n",
    "    # add an attribute for visualization convenience\n",
    "    model.CLASSES = datasets[0].CLASSES\n",
    "    train_detector(\n",
    "        model,\n",
    "        datasets,\n",
    "        cfg,\n",
    "        distributed=distributed,\n",
    "        validate=(not args.no_validate),\n",
    "        timestamp=timestamp,\n",
    "        meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b3a8984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 04:43:09,802 - mmdet - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]\n",
      "CUDA available: False\n",
      "GCC: gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "PyTorch: 1.11.0+cu115\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.5, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "TorchVision: 0.12.0+cu115\n",
      "OpenCV: 4.8.0\n",
      "MMCV: 1.4.0\n",
      "MMCV Compiler: GCC 11.3\n",
      "MMCV CUDA Compiler: not available\n",
      "MMDetection: 2.19.1+03b60c4\n",
      "------------------------------------------------------------\n",
      "\n",
      "2023-08-01 04:43:10,952 - mmdet - INFO - Distributed training: False\n",
      "2023-08-01 04:43:11,780 - mmdet - INFO - Config:\n",
      "norm_cfg = dict(type='BN', requires_grad=True, momentum=0.1)\n",
      "transformer_norm_cfg = dict(type='LN', eps=1e-06)\n",
      "proj_drop_rate = 0.1\n",
      "model = dict(\n",
      "    type='CascadeRCNN',\n",
      "    backbone=dict(\n",
      "        type='HRFuserHRFormerBased',\n",
      "        norm_cfg=dict(type='BN', requires_grad=True, momentum=0.1),\n",
      "        transformer_norm_cfg=dict(type='LN', eps=1e-06),\n",
      "        norm_eval=False,\n",
      "        drop_path_rate=0.0,\n",
      "        num_fused_modalities=2,\n",
      "        extra=dict(\n",
      "            LidarStageA=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=1,\n",
      "                block='BOTTLENECK',\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(64, )),\n",
      "            ModFusionA=dict(\n",
      "                block='MWCA',\n",
      "                with_act=True,\n",
      "                with_pre_act=False,\n",
      "                drop_path=0.2,\n",
      "                num_branches=2,\n",
      "                window_sizes=(7, 7),\n",
      "                num_heads=(1, 2),\n",
      "                mlp_ratios=(4, 4),\n",
      "                num_channels=(18, 36),\n",
      "                proj_drop_rate=0.1),\n",
      "            LidarStageB=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=1,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, ),\n",
      "                num_heads=(1, ),\n",
      "                mlp_ratios=(4, ),\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(18, )),\n",
      "            ModFusionB=dict(\n",
      "                block='MWCA',\n",
      "                with_act=True,\n",
      "                with_pre_act=False,\n",
      "                drop_path=0.2,\n",
      "                num_branches=3,\n",
      "                window_sizes=(7, 7, 7),\n",
      "                num_heads=(1, 2, 4),\n",
      "                mlp_ratios=(4, 4, 4),\n",
      "                num_channels=(18, 36, 72),\n",
      "                proj_drop_rate=0.1),\n",
      "            LidarStageC=dict(\n",
      "                num_modules=3,\n",
      "                num_branches=1,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, ),\n",
      "                num_heads=(1, ),\n",
      "                mlp_ratios=(4, ),\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(18, )),\n",
      "            ModFusionC=dict(\n",
      "                block='MWCA',\n",
      "                with_act=True,\n",
      "                with_pre_act=False,\n",
      "                drop_path=0.2,\n",
      "                num_branches=4,\n",
      "                window_sizes=(7, 7, 7, 7),\n",
      "                num_heads=(1, 2, 4, 8),\n",
      "                mlp_ratios=(4, 4, 4, 4),\n",
      "                num_channels=(18, 36, 72, 144),\n",
      "                proj_drop_rate=0.1),\n",
      "            LidarStageD=None,\n",
      "            stage1=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=1,\n",
      "                block='BOTTLENECK',\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(64, )),\n",
      "            stage2=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=2,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, 7),\n",
      "                num_heads=(1, 2),\n",
      "                mlp_ratios=(4, 4),\n",
      "                num_blocks=(2, 2),\n",
      "                num_channels=(18, 36),\n",
      "                in_module_fusion=False),\n",
      "            stage3=dict(\n",
      "                num_modules=3,\n",
      "                num_branches=3,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, 7, 7),\n",
      "                num_heads=(1, 2, 4),\n",
      "                mlp_ratios=(4, 4, 4),\n",
      "                num_blocks=(2, 2, 2),\n",
      "                num_channels=(18, 36, 72),\n",
      "                in_module_fusion=False),\n",
      "            stage4=dict(\n",
      "                num_modules=2,\n",
      "                num_branches=4,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, 7, 7, 7),\n",
      "                num_heads=(1, 2, 4, 8),\n",
      "                mlp_ratios=(4, 4, 4, 4),\n",
      "                num_blocks=(2, 2, 2, 2),\n",
      "                num_channels=(18, 36, 72, 144),\n",
      "                in_module_fusion=False))),\n",
      "    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(\n",
      "            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='CascadeRoIHead',\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[1, 0.5, 0.25],\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=10,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=10,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=10,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n",
      "        ]),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=0,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=2000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.6,\n",
      "                    neg_iou_thr=0.6,\n",
      "                    min_pos_iou=0.6,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.7,\n",
      "                    min_pos_iou=0.7,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)\n",
      "        ]),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/nuscenes/'\n",
      "class_names = [\n",
      "    'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',\n",
      "    'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "]\n",
      "classes = [\n",
      "    'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',\n",
      "    'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "]\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "lidar_norm_cfg = dict(\n",
      "    mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "    std=[2.5538357826888602, 3.7345728854535643, 0.2815488539921788],\n",
      "    to_rgb=False)\n",
      "radar_norm_cfg = dict(\n",
      "    mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "    std=[3.219927182957935, 0.7240392925308506, 0.11561270078715341],\n",
      "    to_rgb=False)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile', to_float32=True),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='lidar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['rih']),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "        std=[2.5538357826888602, 3.7345728854535643, 0.2815488539921788],\n",
      "        to_rgb=False,\n",
      "        keys=['lidar_img'],\n",
      "        sensor_type='lidar'),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='radar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['riv']),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "        std=[3.219927182957935, 0.7240392925308506, 0.11561270078715341],\n",
      "        to_rgb=False,\n",
      "        keys=['radar_img'],\n",
      "        sensor_type='radar'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_visibility=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=(640, 360),\n",
      "        keep_ratio=True,\n",
      "        skip_keys=['lidar_img', 'radar_img']),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True,\n",
      "        keys=['img']),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(\n",
      "        type='RandomDrop',\n",
      "        p=[0.2, 0.2, 0.2],\n",
      "        keys=['img', 'lidar_img', 'radar_img']),\n",
      "    dict(\n",
      "        type='DefaultFormatBundle',\n",
      "        sensor_keys=['img', 'lidar_img', 'radar_img']),\n",
      "    dict(\n",
      "        type='Collect',\n",
      "        keys=['img', 'lidar_img', 'radar_img', 'gt_bboxes', 'gt_labels'],\n",
      "        meta_keys=('filename', 'ori_filename', 'ori_shape', 'img_shape',\n",
      "                   'pad_shape', 'scale_factor', 'flip', 'flip_direction',\n",
      "                   'img_norm_cfg', 'lidar_ori_shape', 'lidar_norm_cfg',\n",
      "                   'radar_ori_shape', 'radar_norm_cfg'))\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile', to_float32=True),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='lidar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['rih']),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='radar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['riv']),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(640, 360),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                std=[\n",
      "                    2.5538357826888602, 3.7345728854535643, 0.2815488539921788\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['lidar_img'],\n",
      "                sensor_type='lidar'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                std=[\n",
      "                    3.219927182957935, 0.7240392925308506, 0.11561270078715341\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['radar_img'],\n",
      "                sensor_type='radar'),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                keep_ratio=True,\n",
      "                skip_keys=['lidar_img', 'radar_img']),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True,\n",
      "                keys=['img']),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img', 'lidar_img', 'radar_img']),\n",
      "            dict(type='Collect', keys=['img', 'lidar_img', 'radar_img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=3,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=[\n",
      "            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n",
      "            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "        ],\n",
      "        data_root='data/nuscenes/',\n",
      "        ann_file='nuscenes_infos_train_mono3d.coco.json',\n",
      "        img_prefix='',\n",
      "        lidar_prefix='',\n",
      "        radar_prefix='',\n",
      "        lidar_img_mode=True,\n",
      "        radar_img_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='lidar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['rih']),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                std=[\n",
      "                    2.5538357826888602, 3.7345728854535643, 0.2815488539921788\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['lidar_img'],\n",
      "                sensor_type='lidar'),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='radar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['riv']),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                std=[\n",
      "                    3.219927182957935, 0.7240392925308506, 0.11561270078715341\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['radar_img'],\n",
      "                sensor_type='radar'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_visibility=True),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=(640, 360),\n",
      "                keep_ratio=True,\n",
      "                skip_keys=['lidar_img', 'radar_img']),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True,\n",
      "                keys=['img']),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(\n",
      "                type='RandomDrop',\n",
      "                p=[0.2, 0.2, 0.2],\n",
      "                keys=['img', 'lidar_img', 'radar_img']),\n",
      "            dict(\n",
      "                type='DefaultFormatBundle',\n",
      "                sensor_keys=['img', 'lidar_img', 'radar_img']),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=[\n",
      "                    'img', 'lidar_img', 'radar_img', 'gt_bboxes', 'gt_labels'\n",
      "                ],\n",
      "                meta_keys=('filename', 'ori_filename', 'ori_shape',\n",
      "                           'img_shape', 'pad_shape', 'scale_factor', 'flip',\n",
      "                           'flip_direction', 'img_norm_cfg', 'lidar_ori_shape',\n",
      "                           'lidar_norm_cfg', 'radar_ori_shape',\n",
      "                           'radar_norm_cfg'))\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=[\n",
      "            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n",
      "            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "        ],\n",
      "        data_root='data/nuscenes/',\n",
      "        ann_file='nuscenes_infos_val_mono3d.coco.json',\n",
      "        img_prefix='',\n",
      "        lidar_prefix='',\n",
      "        radar_prefix='',\n",
      "        lidar_img_mode=True,\n",
      "        radar_img_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='lidar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['rih']),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='radar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['riv']),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(640, 360),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                        std=[\n",
      "                            2.5538357826888602, 3.7345728854535643,\n",
      "                            0.2815488539921788\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['lidar_img'],\n",
      "                        sensor_type='lidar'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                        std=[\n",
      "                            3.219927182957935, 0.7240392925308506,\n",
      "                            0.11561270078715341\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['radar_img'],\n",
      "                        sensor_type='radar'),\n",
      "                    dict(\n",
      "                        type='Resize',\n",
      "                        keep_ratio=True,\n",
      "                        skip_keys=['lidar_img', 'radar_img']),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True,\n",
      "                        keys=['img']),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(\n",
      "                        type='ImageToTensor',\n",
      "                        keys=['img', 'lidar_img', 'radar_img']),\n",
      "                    dict(\n",
      "                        type='Collect', keys=['img', 'lidar_img', 'radar_img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=[\n",
      "            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n",
      "            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "        ],\n",
      "        data_root='data/nuscenes/',\n",
      "        ann_file='nuscenes_infos_val_mono3d.coco.json',\n",
      "        img_prefix='',\n",
      "        lidar_prefix='',\n",
      "        radar_prefix='',\n",
      "        lidar_img_mode=True,\n",
      "        radar_img_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='lidar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['rih']),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='radar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['riv']),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(640, 360),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                        std=[\n",
      "                            2.5538357826888602, 3.7345728854535643,\n",
      "                            0.2815488539921788\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['lidar_img'],\n",
      "                        sensor_type='lidar'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                        std=[\n",
      "                            3.219927182957935, 0.7240392925308506,\n",
      "                            0.11561270078715341\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['radar_img'],\n",
      "                        sensor_type='radar'),\n",
      "                    dict(\n",
      "                        type='Resize',\n",
      "                        keep_ratio=True,\n",
      "                        skip_keys=['lidar_img', 'radar_img']),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True,\n",
      "                        keys=['img']),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(\n",
      "                        type='ImageToTensor',\n",
      "                        keys=['img', 'lidar_img', 'radar_img']),\n",
      "                    dict(\n",
      "                        type='Collect', keys=['img', 'lidar_img', 'radar_img'])\n",
      "                ])\n",
      "        ],\n",
      "        samples_per_gpu=1))\n",
      "evaluation = dict(interval=1, metric='bbox')\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=0.0003,\n",
      "    betas=(0.9, 0.999),\n",
      "    weight_decay=0.01,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[8, 11])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=12)\n",
      "seed = 0\n",
      "work_dir = './work_dirs/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_bn'\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 04:43:11,781 - mmdet - INFO - Set random seed to 0, deterministic: False\n",
      "2023-08-01 04:43:12,445 - mmdet - INFO - initialize HRFuserHRFormerBased with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]\n",
      "2023-08-01 04:43:12,800 - mmdet - INFO - initialize HRFPN with init_cfg {'type': 'Caffe2Xavier', 'layer': 'Conv2d'}\n",
      "2023-08-01 04:43:12,823 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}\n",
      "2023-08-01 04:43:12,834 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n",
      "2023-08-01 04:43:12,952 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n",
      "2023-08-01 04:43:13,062 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# add an attribute for visualization convenience\u001b[39;00m\n\u001b[1;32m     93\u001b[0m model\u001b[38;5;241m.\u001b[39mCLASSES \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mCLASSES\n\u001b[0;32m---> 94\u001b[0m \u001b[43mtrain_detector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_validate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - James Cook University/Desktop/COMP9444/project/HRFuser/mmdet/apis/train.py:124\u001b[0m, in \u001b[0;36mtrain_detector\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    117\u001b[0m     model \u001b[38;5;241m=\u001b[39m MMDistributedDataParallel(\n\u001b[1;32m    118\u001b[0m         model\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m    119\u001b[0m         device_ids\u001b[38;5;241m=\u001b[39m[torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()],\n\u001b[1;32m    120\u001b[0m         broadcast_buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m         find_unused_parameters\u001b[38;5;241m=\u001b[39mfind_unused_parameters)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     model \u001b[38;5;241m=\u001b[39m MMDataParallel(\n\u001b[0;32m--> 124\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, device_ids\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mgpu_ids)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# build runner\u001b[39;00m\n\u001b[1;32m    127\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m build_optimizer(model, cfg\u001b[38;5;241m.\u001b[39moptimizer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d9427",
   "metadata": {},
   "source": [
    "# 5. Discussion\n",
    "\n",
    "### YOLOv8s Discussion\n",
    "#### System Performance\n",
    "System Specifications: We have trained YOLOv8 model through Google Colab with NVIDIA A100-SXM4-40GB\n",
    "\n",
    "#### Dataset\n",
    "Due to time limit, we have used a mini-training dataset (2049 images) randomly extracted from nuImages which are all from camera with 2D annotations. The proportion of daytime and nigttiem images in the mini dataset remained consistent with that of overall dataset.\n",
    "#### Training Specifications\n",
    "Training parameter have been pre-tuned by the developer:\n",
    "- Optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n",
    "- batch: 16\n",
    "- IoU: 0.7\n",
    "And we set image size = 1280, which has a better result.\n",
    "\n",
    "#### Training Time\n",
    "The training process of YOLOv8 on the mini dataset would take 0.230 hours. When we added epoch size to 150, our model stopped training at epoch 75, since there was no improvement in the last 50 epochs.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_training _time.png\" />\n",
    "</div>\n",
    "\n",
    "And the best result was observed at epoch 25, so we used epoch size = 25. As a result, the training time has decreased to 0.08 hours.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/yolov8_training_time_25_epoches.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "The nuImages sample dataset contains approximately 40,000 images in total. But after downloading, we got around 20,000 images as training set. Using all of these images for training the model would result in excessively long training times. Apart from the provided all_sample dataset, the official source also offers a mini dataset with only 50 images. However, if we use the mini dataset to train the model, the small sample size and insufficient representation of certain categories could lead to inaccurate learning for those categories and over-fitting as well.\n",
    "\n",
    "In order to mitigate the challenges associated with dataset size, we constructed a dataset by randomly sampling 350 images from each camera and manually adjusted the distribution of daytime and nighttime images within our mini dataset from nuImage. However, this approach may still lead to imbalanced proportions of different classes in comparison to the full dataset, potentially influencing our prediction results.\n",
    "\n",
    "### HRFUser Model Discussion\n",
    "\n",
    "\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We train this data on another machine which is a Linux-Sub-System Machine but does not have GPU onboard, the training has to be on CPU. We have to reduce the batch size and the dataset. The training would took ages for this to happen so we have to make use of pre-trained weighted\n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of Nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "As we have insufficient training resources, for this device working on this model, we have to make use of the pre-trained weight provided by the Research Paper\n",
    "\n",
    "#### Model Architecture\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "#### Training Time\n",
    "It would take ages, approximately more than 8 hours on the device working on this. But to use pre-trained weight, it would cost us approximately an hour to perform.\n",
    "\n",
    "<div style=\"margin-top: 10px; margin-bottom: 10px;\">\n",
    "<img src=\"./images/train_time_HR_Fuser.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "The challenge for this model is that it makes use of the mmdet library and mmcv but the running environment on the paper provided is only suitable with a Linux running environment. We would have to create a WSL ( Window Subsystem for Linux) and then run the project on it. Also, WSL cannot connect and refer directly with the Window's CUDA and GPU, we have to modify the code for it to accept CPU train on Pytorch. As CPU train is very limited, we have to fully reduced the batch size to 1 and then perform training on a small MiniDataset instead of a huge one. Also, making use of a pre-trained weight would save us much time rather than training the whole process.\n",
    "\n",
    "Overall HRuser actually better the performance on low-light detection since it was able to fuse all the data input such as camera and sensors together for the train. Thus, improve significantly the detection on varies foggy and low-light environment.\n",
    "\n",
    "<div style=\"margin-top: 10px; margin-bottom: 10px;\">\n",
    "<img src=\"./images/HR_Fuser_discussion.png\" />\n",
    "</div>\n",
    "\n",
    "### 2DPASS Discussion\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We have trained the 2DPASS model on a Nvidia 4060 laptop graphics card with 16 gigabytes of RAM. \n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "Training parameters have been pre-tuned by the developers as:\n",
    "- Learning Rate: 0.24\n",
    "- Optimizer: SGD\n",
    "- Momentum: 0.9\n",
    "- Weight Decay: 1.0e-4\n",
    "\n",
    "#### Model Architecture\n",
    "This model significantly improves upon simple image computer vision neural networks, as 2DPASS introduces lidar detection combined with the use of image. This more accurately detects the existence and classification of the object even in low luminosity environments.\n",
    "\n",
    "#### Training Time\n",
    "The training process of our model on the mini-dataset took approximately 5 hours, which is due to our computer’s limited memory as it was only able to manage a batch training size of one. Also, due to the limited variety in the mini-dataset, we observed that the val/mIoU failed to show improvements over the last 50 records, which shows that a lot of the computation towards the end of training did not achieve any notable performance improvements.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/train_time.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "Originally running the model on the whole 80 gigabytes data requires too much computational power and time, so we resorted to using the mini-training set instead, which was much faster to train.\n",
    "\n",
    "Training on a much smaller dataset could potentially introduce overfitting of data and lead to inaccurate results, in this case we have used their pre-trained model to compare results before drawing conclusions.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/overfit.png\" />\n",
    "</div>\n",
    "The above is the result from testing the model trained with the mini-dataset, and here we can clearly see a case of overfitting where all vehicle like objects are recognised as cars explaining the high accuracy in car predictions and basically 0% accuracy in all other vehicles detections.\n",
    "\n",
    "Our main challenges occurred within our limited ability to modify the model, as the training time even on a much smaller dataset took up to five hours. To tackle this problem, we have introduced early-stopping of the training, where if we do not see noticeable improvements on the mIoU(mean intersection over Union) value over five epochs of training we will manually exit the training. However, finding a sweet spot for the improvement was difficult and is hard to optimise. Moreover, as training is also dependent on the distribution of the dataset, it is uncertain how much the model will learn from processing different data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ef7c0",
   "metadata": {},
   "source": [
    "### References (To be cleaned up later)\n",
    "\n",
    "#### General Research:\n",
    "\n",
    "Darya Paspelava. Computer Vision Object Detection: Challenges Faced. Retrieved July 28, 2023, from https://www.exposit.com/blog/computer-vision-object-detection-challenges-faced/\n",
    "\n",
    "Liu, Z., Cai, Y., Wang, H. et al. Surrounding Objects Detection and Tracking for Autonomous Driving Using LiDAR and Radar Fusion. Chin. J. Mech. Eng. 34, 117 (2021). https://doi.org/10.1186/s10033-021-00630-y\n",
    "\n",
    "Gaudenz Boesch. Object Detection in 2023: The Definitive Guide. Retrieved July 26, 2023, from https://viso.ai/deep-learning/object-detection/\n",
    "\n",
    "#### Nuscenes Dataset:\n",
    "Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., Beijbom, O., no year. nuScenes: A multimodal dataset for autonomous driving. Available at: https://doi.org/10.48550/arXiv.1903.11027\n",
    "\n",
    "\n",
    "#### HRFuser:\n",
    "Broedermann, T., Sakaridis, C., Dai, D. and Van Gool, L., 2023. HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection. ETH Zurich/MPI for Informatics/KU Leuven. Available at: https://doi.org/10.48550/arXiv.2206.15157\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
