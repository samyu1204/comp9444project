{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c77fe9",
   "metadata": {},
   "source": [
    "# COMP9444 Group Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76861f",
   "metadata": {},
   "source": [
    "## 1. Introduction, Motivation, and/or Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b2962",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Our project aims to leverage computer vision neural network to improve object detection of images during both daytime and nighttime environments. The ability to accurately detect and recognize objects in varying lighting condition has become crucial for the functionalities of many modern day applications; some examples would be autonomous vehicles, surveillance and security systems.\n",
    "\n",
    "Consider the two images below. It is imperative that everything in left image is very easy to identify, and when contrasted to the image on the right it really highlights just how much harder it is to identify objects with low luminosity.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"https://www.exposit.com/wp-content/webp-express/webp-images/doc-root/wp-content/uploads/2021/04/Illumination_conditions_as_a_challenge_of_comp.width-800.jpg.webp\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Motivation\n",
    "Modern day computer vision neural networks often fail to perform well in nighttime object detection (inaccurate detection of objects in low luminosity environments). Nighttime environment factors like shadow, limited luminosity, and visibility makes it challenging for the network to classify objects. With this problem, it can hinder the effectiveness and safety of pre-existing computer vision applications like surveillance, which requires all day monitoring.\n",
    "\n",
    "Researchers have made advancements in enhancing accuracy for low-light detection. An example is the REDI low-light enhancement algorithm, which effectively filters noise in low-light conditions and performs detection on the resulting image.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/lowlight.png\" />\n",
    "</div>\n",
    "\n",
    "Here (a) through to (d) are stages of REDI algorithm filtering. However, there are many downsides to this algorithm like loss of details, over-correction, and high computational cost. This would pose a challenge as it would add extra complexity and computational stress on existing models.\n",
    "\n",
    "Solving day/night object detection will definitely bring significant enhancements in the real world, and some key areas of improvements are autonomous driving, surveillance and security systems. This is not only an exciting technical challenge for researchers, but also has the potential to open up new possibilities for neural network computer vision advancements.\n",
    "\n",
    "### Problem Statements\n",
    "Key challenges that requires to be address by our models are:\n",
    "1. The model requires to handle varying levels of brightness within the image.\n",
    "2. Removing noise from nighttime image, as image taken at night might have more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e923798",
   "metadata": {},
   "source": [
    "## 2. Exploration Analysis or Data or RL Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f399faf",
   "metadata": {},
   "source": [
    "## 3. Models and/or Methods\n",
    "\n",
    "### 2DPASS \n",
    "Link to paper: https://arxiv.org/pdf/2210.04208.pdf\n",
    "\n",
    "#### Model Introduction\n",
    "This model is an Assisted Semantic Segmentation method that boosts the representation learning on point clouds. A notable advantage of this model is that \n",
    "Advantages of this model is that it does not require strict pair data alignments between the camera and LiDAR data. \n",
    "\n",
    "The 2DPASS method leverages an auxiliary model fusion and multi-scale fusion to single knowledge distillation (MSFSKD) to acquire richer semantic and structural information from the multi-modal data. This is a significant improvement over baseline models where models only use point cloud.\n",
    "\n",
    "\n",
    "### HRFUser\n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/2206.15157.pdf\n",
    "\n",
    "\n",
    "\n",
    "### Model Introduction\n",
    "\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "While numerous recent research focus on fusing specific pairs of sensors—such as camera with lidar or radar—by leveraging architectural components relevant to the investigated context, the literature lacks a general and modular sensor fusion architecture. We have HRFuser, a modular architecture for multi-modal 2D object identification. It multiresolutionly integrates numerous sensors and scales to an indefinite number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "HRFuser have a slight special architecture being shown as follow:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-architecture.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "Because of extended layer of input, HRUser results in a better training of combination Data on not just cameras but also multiple type of sensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254fdf1",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 2DPASS Results\n",
    "\n",
    "#### 2DPASS Trained on Mini-Dataset\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/mini.png\" />\n",
    "</div>\n",
    "\n",
    "#### 2DPASS Pretrained Model\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/pretrained.png\" />\n",
    "</div>\n",
    "\n",
    "#### Model Results\n",
    "| Model                | mIoU | Accuracy |\n",
    "|----------------------|------|----------|\n",
    "| 2DPASS (Mini-dataset)| 36%  | 56%      |\n",
    "| 2DPASS (Pretrained)  | 81%  | 63%      |\n",
    "\n",
    "Major improvements in accuracy and mIoU are both significant for the pretrained model which was initially trained on the full dataset. Note, that this result is worse than the one displayed in the paper as their model was trained with additional validation set and using instance-level augmentation.\n",
    "\n",
    "#### Epoch Training Steps\n",
    "NOTE: X-axis is number of epoch.\n",
    "##### mIoU vs Epoch \n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/miou_r.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "#### Best mIoU vs Epoch\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/miou.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "From the mIoU curves and best mIoU curve(smoothened out), we see that around 8000 epoch there are no significant improves in the mIoU value, emphasizing that further training after 8000 epoch does not improve the model, and could lead to overfitting existing data.\n",
    "\n",
    "##### Accuracy vs Epoch \n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/accuracy.png\" width=\"700px\" />\n",
    "</div>\n",
    "The accuracy during the training of the model behaves similarly to the mIoU curve as optimum accuracy is reached around 8000 epoche\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### HRFUser Results\n",
    "\n",
    "#### General sample images results:\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser_result.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR-Fuser-Result-2.png\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Images output from Nuscene MiniDataset after train (note this is only a few out of more than 500 results):\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuser_Result.png\" width=\"1000px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "##### Front Camera:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Front-Camera/FC2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "##### Back Camera:\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Camera/BC2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "##### Back Left Camera:\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Left-Camera/BLC2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "##### Back Right Camera:\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR_Camera2.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HRFuseroutput/Back-Right-Camera/BR-Camera1.jpg\" width=\"700px\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d9427",
   "metadata": {},
   "source": [
    "## 5. Discussion\n",
    "### 2DPASS Discussion\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We have trained the 2DPASS model on a Nvidia 4060 laptop graphics card with 16 gigabytes of RAM. \n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "Training parameters have been pre-tuned by the developers as:\n",
    "- Learning Rate: 0.24\n",
    "- Optimizer: SGD\n",
    "- Momentum: 0.9\n",
    "- Weight Decay: 1.0e-4\n",
    "\n",
    "#### Model Architecture\n",
    "This model significantly improves upon simple image computer vision neural networks, as 2DPASS introduces lidar detection combined with the use of image. This more accurately detects the existence and classification of the object even in low luminosity environments.\n",
    "\n",
    "#### Training Time\n",
    "The training process of our model on the mini-dataset took approximately 5 hours, which is due to our computer’s limited memory as it was only able to manage a batch training size of one. Also, due to the limited variety in the mini-dataset, we observed that the val/mIoU failed to show improvements over the last 50 records, which shows that a lot of the computation towards the end of training did not achieve any notable performance improvements.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/train_time.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "Originally running the model on the whole 80 gigabytes data requires too much computational power and time, so we resorted to using the mini-training set instead, which was much faster to train.\n",
    "\n",
    "Training on a much smaller dataset could potentially introduce overfitting of data and lead to inaccurate results, in this case we have used their pre-trained model to compare results before drawing conclusions.\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/overfit.png\" />\n",
    "</div>\n",
    "The above is the result from testing the model trained with the mini-dataset, and here we can clearly see a case of overfitting where all vehicle like objects are recognised as cars explaining the high accuracy in car predictions and basically 0% accuracy in all other vehicles detections.\n",
    "\n",
    "Our main challenges occurred within our limited ability to modify the model, as the training time even on a much smaller dataset took up to five hours. To tackle this problem, we have introduced early-stopping of the training, where if we do not see noticeable improvements on the mIoU(mean intersection over Union) value over five epochs of training we will manually exit the training. However, finding a sweet spot for the improvement was difficult and is hard to optimise. Moreover, as training is also dependent on the distribution of the dataset, it is uncertain how much the model will learn from processing different data.\n",
    "\n",
    "\n",
    "\n",
    "### HRFUser Model Discussion\n",
    "\n",
    "\n",
    "#### System Performance:\n",
    "System Specifications:\n",
    "We train this data on another machine which is a Linux-Sub-System Machine but does not have GPU onboard, the training has to be on CPU. We have to reduce the batch size and the dataset. The training would took ages for this to happen so we have to make use of pre-trained weighted\n",
    "\n",
    "#### Dataset:\n",
    "For the interest of time we have used the mini-training dataset of Nuscenes which is around 6 gigabytes compared to the 80 gigabytes full dataset.\n",
    "\n",
    "#### Training Specifications\n",
    "Training batch size had to be limited to a size of 1 as any batch sizes larger than this would cause insufficient memory errors.\n",
    "As we have insufficient training resources, for this device working on this model, we have to make use of the pre-trained weight provided by the Research Paper\n",
    "\n",
    "#### Model Architecture\n",
    "HRFuser is a multi-resolution sensor fusion architecture that easily scales to any number of input modalities. HRFuser is built on cutting-edge high-resolution networks for image-only dense prediction and includes a new multi-window cross-attention block to conduct fusion of many modalities at multiple resolutions.\n",
    "\n",
    "#### Training Time\n",
    "It would take ages, approximately more than 8 hours on the device working on this. But to use pre-trained weight, it would cost us approximately an hour to perform.\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/train_time_HR_Fuser.png\" />\n",
    "</div>\n",
    "\n",
    "#### Challenges and Solutions\n",
    "The challenge for this model is that it makes use of the mmdet library and mmcv but the running environment on the paper provided is only suitable with a Linux running environment. We would have to create a WSL ( Window Subsystem for Linux) and then run the project on it. Also, WSL cannot connect and refer directly with the Window's CUDA and GPU, we have to modify the code for it to accept CPU train on Pytorch. As CPU train is very limited, we have to fully reduced the batch size to 1 and then perform training on a small MiniDataset instead of a huge one. Also, making use of a pre-trained weight would save us much time rather than training the whole process.\n",
    "\n",
    "Overall HRuser actually better the performance on low-light detection since it was able to fuse all the data input such as camera and sensors together for the train. Thus, improve significantly the detection on varies foggy and low-light environment.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<img src=\"./images/HR_Fuser_discussion.png\" />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac898b0",
   "metadata": {},
   "source": [
    "## HRFuser code demo on a pre trained weight with output file generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e10e9",
   "metadata": {},
   "source": [
    "#### Note: Since the file is loaded with images, you can check it up above as I already put it as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce02e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.utils import get_git_hash\n",
    "\n",
    "from mmdet import __version__\n",
    "from mmdet.apis import init_random_seed, set_random_seed, train_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.utils import collect_env, get_root_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a527dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "arguments = ['./HRFuser_config/hrfuser/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_bn.py', # batch-norm\n",
    "             './checkpoints/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_latest.pth',\n",
    "             '--cfg-options', 'data.test.samples_per_gpu=1',\n",
    "             '--show-dir', 'demo/output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9284d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a detector')\n",
    "    parser.add_argument('config', help='train config file path')\n",
    "    parser.add_argument('--work-dir', help='the dir to save logs and models')\n",
    "    parser.add_argument(\n",
    "        '--resume-from', help='the checkpoint file to resume from')\n",
    "    parser.add_argument(\n",
    "        '--no-validate',\n",
    "        action='store_true',\n",
    "        help='whether not to evaluate the checkpoint during training')\n",
    "    group_gpus = parser.add_mutually_exclusive_group()\n",
    "    group_gpus.add_argument(\n",
    "        '--gpus',\n",
    "        type=int,\n",
    "        help='number of gpus to use '\n",
    "        '(only applicable to non-distributed training)')\n",
    "    group_gpus.add_argument(\n",
    "        '--gpu-ids',\n",
    "        type=int,\n",
    "        nargs='+',\n",
    "        help='ids of gpus to use '\n",
    "        '(only applicable to non-distributed training)')\n",
    "    parser.add_argument('--seed', type=int, default=None, help='random seed')\n",
    "    parser.add_argument(\n",
    "        '--deterministic',\n",
    "        action='store_true',\n",
    "        help='whether to set deterministic options for CUDNN backend.')\n",
    "    parser.add_argument(\n",
    "        '--options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file (deprecate), '\n",
    "        'change to --cfg-options instead.')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file. If the value to '\n",
    "        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n",
    "        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n",
    "        'Note that the quotation marks are necessary and that no white space '\n",
    "        'is allowed.')\n",
    "    parser.add_argument(\n",
    "        '--launcher',\n",
    "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
    "        default='none',\n",
    "        help='job launcher')\n",
    "    parser.add_argument('--local_rank', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args(arguments)\n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "\n",
    "    if args.options and args.cfg_options:\n",
    "        raise ValueError(\n",
    "            '--options and --cfg-options cannot be both '\n",
    "            'specified, --options is deprecated in favor of --cfg-options')\n",
    "    if args.options:\n",
    "        warnings.warn('--options is deprecated in favor of --cfg-options')\n",
    "        args.cfg_options = args.options\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc47b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "    # set cudnn_benchmark\n",
    "    if cfg.get('cudnn_benchmark', False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # work_dir is determined in this priority: CLI > segment in file > filename\n",
    "    if args.work_dir is not None:\n",
    "        # update configs according to CLI args if args.work_dir is not None\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        # use config filename as default work_dir if cfg.work_dir is None\n",
    "        cfg.work_dir = osp.join('./work_dirs',\n",
    "                                osp.splitext(osp.basename(args.config))[0])\n",
    "    if args.resume_from is not None:\n",
    "        cfg.resume_from = args.resume_from\n",
    "    if args.gpu_ids is not None:\n",
    "        cfg.gpu_ids = args.gpu_ids\n",
    "    else:\n",
    "        cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    if args.launcher == 'none':\n",
    "        distributed = False\n",
    "    else:\n",
    "        distributed = True\n",
    "        init_dist(args.launcher, **cfg.dist_params)\n",
    "        # re-set gpu_ids with distributed training mode\n",
    "        _, world_size = get_dist_info()\n",
    "        cfg.gpu_ids = range(world_size)\n",
    "\n",
    "    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "    # create work_dir\n",
    "    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "    # dump config\n",
    "    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n",
    "    # init the logger before other steps\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
    "    logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
    "\n",
    "    # init the meta dict to record some important information such as\n",
    "    # environment info and seed, which will be logged\n",
    "    meta = dict()\n",
    "    # log env info\n",
    "    env_info_dict = collect_env()\n",
    "    env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n",
    "    dash_line = '-' * 60 + '\\n'\n",
    "    logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "                dash_line)\n",
    "    meta['env_info'] = env_info\n",
    "    meta['config'] = cfg.pretty_text\n",
    "    # log some basic info\n",
    "    logger.info(f'Distributed training: {distributed}')\n",
    "    logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "    # set random seeds\n",
    "    if 'seed' in cfg.keys() and cfg.seed is not None:\n",
    "        seed = cfg.seed\n",
    "    else:\n",
    "        seed = init_random_seed(args.seed)\n",
    "    logger.info(f'Set random seed to {seed}, '\n",
    "                f'deterministic: {args.deterministic}')\n",
    "    #set_random_seed(seed, deterministic=args.deterministic)\n",
    "    cfg.seed = seed\n",
    "    meta['seed'] = seed\n",
    "    meta['exp_name'] = osp.basename(args.config)\n",
    "\n",
    "    model = build_detector(\n",
    "        cfg.model,\n",
    "        train_cfg=cfg.get('train_cfg'),\n",
    "        test_cfg=cfg.get('test_cfg'))\n",
    "    model.init_weights()\n",
    "\n",
    "    datasets = [build_dataset(cfg.data.train)]\n",
    "    if len(cfg.workflow) == 2:\n",
    "        val_dataset = copy.deepcopy(cfg.data.val)\n",
    "        val_dataset.pipeline = cfg.data.train.pipeline\n",
    "        datasets.append(build_dataset(val_dataset))\n",
    "    if cfg.checkpoint_config is not None:\n",
    "        # save mmdet version, config file content and class names in\n",
    "        # checkpoints as meta data\n",
    "        cfg.checkpoint_config.meta = dict(\n",
    "            mmdet_version=__version__ + get_git_hash()[:7],\n",
    "            config=cfg.pretty_text,\n",
    "            CLASSES=datasets[0].CLASSES)\n",
    "\n",
    "    # add an attribute for visualization convenience\n",
    "    model.CLASSES = datasets[0].CLASSES\n",
    "    train_detector(\n",
    "        model,\n",
    "        datasets,\n",
    "        cfg,\n",
    "        distributed=distributed,\n",
    "        validate=(not args.no_validate),\n",
    "        timestamp=timestamp,\n",
    "        meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b3a8984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 04:43:09,802 - mmdet - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]\n",
      "CUDA available: False\n",
      "GCC: gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "PyTorch: 1.11.0+cu115\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.5, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "TorchVision: 0.12.0+cu115\n",
      "OpenCV: 4.8.0\n",
      "MMCV: 1.4.0\n",
      "MMCV Compiler: GCC 11.3\n",
      "MMCV CUDA Compiler: not available\n",
      "MMDetection: 2.19.1+03b60c4\n",
      "------------------------------------------------------------\n",
      "\n",
      "2023-08-01 04:43:10,952 - mmdet - INFO - Distributed training: False\n",
      "2023-08-01 04:43:11,780 - mmdet - INFO - Config:\n",
      "norm_cfg = dict(type='BN', requires_grad=True, momentum=0.1)\n",
      "transformer_norm_cfg = dict(type='LN', eps=1e-06)\n",
      "proj_drop_rate = 0.1\n",
      "model = dict(\n",
      "    type='CascadeRCNN',\n",
      "    backbone=dict(\n",
      "        type='HRFuserHRFormerBased',\n",
      "        norm_cfg=dict(type='BN', requires_grad=True, momentum=0.1),\n",
      "        transformer_norm_cfg=dict(type='LN', eps=1e-06),\n",
      "        norm_eval=False,\n",
      "        drop_path_rate=0.0,\n",
      "        num_fused_modalities=2,\n",
      "        extra=dict(\n",
      "            LidarStageA=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=1,\n",
      "                block='BOTTLENECK',\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(64, )),\n",
      "            ModFusionA=dict(\n",
      "                block='MWCA',\n",
      "                with_act=True,\n",
      "                with_pre_act=False,\n",
      "                drop_path=0.2,\n",
      "                num_branches=2,\n",
      "                window_sizes=(7, 7),\n",
      "                num_heads=(1, 2),\n",
      "                mlp_ratios=(4, 4),\n",
      "                num_channels=(18, 36),\n",
      "                proj_drop_rate=0.1),\n",
      "            LidarStageB=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=1,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, ),\n",
      "                num_heads=(1, ),\n",
      "                mlp_ratios=(4, ),\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(18, )),\n",
      "            ModFusionB=dict(\n",
      "                block='MWCA',\n",
      "                with_act=True,\n",
      "                with_pre_act=False,\n",
      "                drop_path=0.2,\n",
      "                num_branches=3,\n",
      "                window_sizes=(7, 7, 7),\n",
      "                num_heads=(1, 2, 4),\n",
      "                mlp_ratios=(4, 4, 4),\n",
      "                num_channels=(18, 36, 72),\n",
      "                proj_drop_rate=0.1),\n",
      "            LidarStageC=dict(\n",
      "                num_modules=3,\n",
      "                num_branches=1,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, ),\n",
      "                num_heads=(1, ),\n",
      "                mlp_ratios=(4, ),\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(18, )),\n",
      "            ModFusionC=dict(\n",
      "                block='MWCA',\n",
      "                with_act=True,\n",
      "                with_pre_act=False,\n",
      "                drop_path=0.2,\n",
      "                num_branches=4,\n",
      "                window_sizes=(7, 7, 7, 7),\n",
      "                num_heads=(1, 2, 4, 8),\n",
      "                mlp_ratios=(4, 4, 4, 4),\n",
      "                num_channels=(18, 36, 72, 144),\n",
      "                proj_drop_rate=0.1),\n",
      "            LidarStageD=None,\n",
      "            stage1=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=1,\n",
      "                block='BOTTLENECK',\n",
      "                num_blocks=(2, ),\n",
      "                num_channels=(64, )),\n",
      "            stage2=dict(\n",
      "                num_modules=1,\n",
      "                num_branches=2,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, 7),\n",
      "                num_heads=(1, 2),\n",
      "                mlp_ratios=(4, 4),\n",
      "                num_blocks=(2, 2),\n",
      "                num_channels=(18, 36),\n",
      "                in_module_fusion=False),\n",
      "            stage3=dict(\n",
      "                num_modules=3,\n",
      "                num_branches=3,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, 7, 7),\n",
      "                num_heads=(1, 2, 4),\n",
      "                mlp_ratios=(4, 4, 4),\n",
      "                num_blocks=(2, 2, 2),\n",
      "                num_channels=(18, 36, 72),\n",
      "                in_module_fusion=False),\n",
      "            stage4=dict(\n",
      "                num_modules=2,\n",
      "                num_branches=4,\n",
      "                block='HRFORMER',\n",
      "                window_sizes=(7, 7, 7, 7),\n",
      "                num_heads=(1, 2, 4, 8),\n",
      "                mlp_ratios=(4, 4, 4, 4),\n",
      "                num_blocks=(2, 2, 2, 2),\n",
      "                num_channels=(18, 36, 72, 144),\n",
      "                in_module_fusion=False))),\n",
      "    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(\n",
      "            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='CascadeRoIHead',\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[1, 0.5, 0.25],\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=10,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=10,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=10,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n",
      "        ]),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=0,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=2000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.6,\n",
      "                    neg_iou_thr=0.6,\n",
      "                    min_pos_iou=0.6,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.7,\n",
      "                    min_pos_iou=0.7,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)\n",
      "        ]),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/nuscenes/'\n",
      "class_names = [\n",
      "    'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',\n",
      "    'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "]\n",
      "classes = [\n",
      "    'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',\n",
      "    'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "]\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "lidar_norm_cfg = dict(\n",
      "    mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "    std=[2.5538357826888602, 3.7345728854535643, 0.2815488539921788],\n",
      "    to_rgb=False)\n",
      "radar_norm_cfg = dict(\n",
      "    mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "    std=[3.219927182957935, 0.7240392925308506, 0.11561270078715341],\n",
      "    to_rgb=False)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile', to_float32=True),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='lidar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['rih']),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "        std=[2.5538357826888602, 3.7345728854535643, 0.2815488539921788],\n",
      "        to_rgb=False,\n",
      "        keys=['lidar_img'],\n",
      "        sensor_type='lidar'),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='radar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['riv']),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "        std=[3.219927182957935, 0.7240392925308506, 0.11561270078715341],\n",
      "        to_rgb=False,\n",
      "        keys=['radar_img'],\n",
      "        sensor_type='radar'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_visibility=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=(640, 360),\n",
      "        keep_ratio=True,\n",
      "        skip_keys=['lidar_img', 'radar_img']),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True,\n",
      "        keys=['img']),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(\n",
      "        type='RandomDrop',\n",
      "        p=[0.2, 0.2, 0.2],\n",
      "        keys=['img', 'lidar_img', 'radar_img']),\n",
      "    dict(\n",
      "        type='DefaultFormatBundle',\n",
      "        sensor_keys=['img', 'lidar_img', 'radar_img']),\n",
      "    dict(\n",
      "        type='Collect',\n",
      "        keys=['img', 'lidar_img', 'radar_img', 'gt_bboxes', 'gt_labels'],\n",
      "        meta_keys=('filename', 'ori_filename', 'ori_shape', 'img_shape',\n",
      "                   'pad_shape', 'scale_factor', 'flip', 'flip_direction',\n",
      "                   'img_norm_cfg', 'lidar_ori_shape', 'lidar_norm_cfg',\n",
      "                   'radar_ori_shape', 'radar_norm_cfg'))\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile', to_float32=True),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='lidar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['rih']),\n",
      "    dict(\n",
      "        type='LoadProjectedSensorImageFile',\n",
      "        sensor_type='radar',\n",
      "        to_float32=True,\n",
      "        color_type='unchanged',\n",
      "        channels=['riv']),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(640, 360),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                std=[\n",
      "                    2.5538357826888602, 3.7345728854535643, 0.2815488539921788\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['lidar_img'],\n",
      "                sensor_type='lidar'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                std=[\n",
      "                    3.219927182957935, 0.7240392925308506, 0.11561270078715341\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['radar_img'],\n",
      "                sensor_type='radar'),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                keep_ratio=True,\n",
      "                skip_keys=['lidar_img', 'radar_img']),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True,\n",
      "                keys=['img']),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img', 'lidar_img', 'radar_img']),\n",
      "            dict(type='Collect', keys=['img', 'lidar_img', 'radar_img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=3,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=[\n",
      "            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n",
      "            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "        ],\n",
      "        data_root='data/nuscenes/',\n",
      "        ann_file='nuscenes_infos_train_mono3d.coco.json',\n",
      "        img_prefix='',\n",
      "        lidar_prefix='',\n",
      "        radar_prefix='',\n",
      "        lidar_img_mode=True,\n",
      "        radar_img_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='lidar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['rih']),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                std=[\n",
      "                    2.5538357826888602, 3.7345728854535643, 0.2815488539921788\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['lidar_img'],\n",
      "                sensor_type='lidar'),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='radar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['riv']),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                std=[\n",
      "                    3.219927182957935, 0.7240392925308506, 0.11561270078715341\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                keys=['radar_img'],\n",
      "                sensor_type='radar'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_visibility=True),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=(640, 360),\n",
      "                keep_ratio=True,\n",
      "                skip_keys=['lidar_img', 'radar_img']),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True,\n",
      "                keys=['img']),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(\n",
      "                type='RandomDrop',\n",
      "                p=[0.2, 0.2, 0.2],\n",
      "                keys=['img', 'lidar_img', 'radar_img']),\n",
      "            dict(\n",
      "                type='DefaultFormatBundle',\n",
      "                sensor_keys=['img', 'lidar_img', 'radar_img']),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=[\n",
      "                    'img', 'lidar_img', 'radar_img', 'gt_bboxes', 'gt_labels'\n",
      "                ],\n",
      "                meta_keys=('filename', 'ori_filename', 'ori_shape',\n",
      "                           'img_shape', 'pad_shape', 'scale_factor', 'flip',\n",
      "                           'flip_direction', 'img_norm_cfg', 'lidar_ori_shape',\n",
      "                           'lidar_norm_cfg', 'radar_ori_shape',\n",
      "                           'radar_norm_cfg'))\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=[\n",
      "            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n",
      "            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "        ],\n",
      "        data_root='data/nuscenes/',\n",
      "        ann_file='nuscenes_infos_val_mono3d.coco.json',\n",
      "        img_prefix='',\n",
      "        lidar_prefix='',\n",
      "        radar_prefix='',\n",
      "        lidar_img_mode=True,\n",
      "        radar_img_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='lidar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['rih']),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='radar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['riv']),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(640, 360),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                        std=[\n",
      "                            2.5538357826888602, 3.7345728854535643,\n",
      "                            0.2815488539921788\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['lidar_img'],\n",
      "                        sensor_type='lidar'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                        std=[\n",
      "                            3.219927182957935, 0.7240392925308506,\n",
      "                            0.11561270078715341\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['radar_img'],\n",
      "                        sensor_type='radar'),\n",
      "                    dict(\n",
      "                        type='Resize',\n",
      "                        keep_ratio=True,\n",
      "                        skip_keys=['lidar_img', 'radar_img']),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True,\n",
      "                        keys=['img']),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(\n",
      "                        type='ImageToTensor',\n",
      "                        keys=['img', 'lidar_img', 'radar_img']),\n",
      "                    dict(\n",
      "                        type='Collect', keys=['img', 'lidar_img', 'radar_img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=[\n",
      "            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n",
      "            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'\n",
      "        ],\n",
      "        data_root='data/nuscenes/',\n",
      "        ann_file='nuscenes_infos_val_mono3d.coco.json',\n",
      "        img_prefix='',\n",
      "        lidar_prefix='',\n",
      "        radar_prefix='',\n",
      "        lidar_img_mode=True,\n",
      "        radar_img_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='lidar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['rih']),\n",
      "            dict(\n",
      "                type='LoadProjectedSensorImageFile',\n",
      "                sensor_type='radar',\n",
      "                to_float32=True,\n",
      "                color_type='unchanged',\n",
      "                channels=['riv']),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(640, 360),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.23277158, 0.31501067, -0.00012928071],\n",
      "                        std=[\n",
      "                            2.5538357826888602, 3.7345728854535643,\n",
      "                            0.2815488539921788\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['lidar_img'],\n",
      "                        sensor_type='lidar'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.19778967, 0.03477772, 0.0025186215],\n",
      "                        std=[\n",
      "                            3.219927182957935, 0.7240392925308506,\n",
      "                            0.11561270078715341\n",
      "                        ],\n",
      "                        to_rgb=False,\n",
      "                        keys=['radar_img'],\n",
      "                        sensor_type='radar'),\n",
      "                    dict(\n",
      "                        type='Resize',\n",
      "                        keep_ratio=True,\n",
      "                        skip_keys=['lidar_img', 'radar_img']),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True,\n",
      "                        keys=['img']),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(\n",
      "                        type='ImageToTensor',\n",
      "                        keys=['img', 'lidar_img', 'radar_img']),\n",
      "                    dict(\n",
      "                        type='Collect', keys=['img', 'lidar_img', 'radar_img'])\n",
      "                ])\n",
      "        ],\n",
      "        samples_per_gpu=1))\n",
      "evaluation = dict(interval=1, metric='bbox')\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=0.0003,\n",
      "    betas=(0.9, 0.999),\n",
      "    weight_decay=0.01,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[8, 11])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=12)\n",
      "seed = 0\n",
      "work_dir = './work_dirs/cascade_rcnn_hrfuser_t_1x_nus_r640_l_r_fusion_bn'\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 04:43:11,781 - mmdet - INFO - Set random seed to 0, deterministic: False\n",
      "2023-08-01 04:43:12,445 - mmdet - INFO - initialize HRFuserHRFormerBased with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]\n",
      "2023-08-01 04:43:12,800 - mmdet - INFO - initialize HRFPN with init_cfg {'type': 'Caffe2Xavier', 'layer': 'Conv2d'}\n",
      "2023-08-01 04:43:12,823 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}\n",
      "2023-08-01 04:43:12,834 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n",
      "2023-08-01 04:43:12,952 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n",
      "2023-08-01 04:43:13,062 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# add an attribute for visualization convenience\u001b[39;00m\n\u001b[1;32m     93\u001b[0m model\u001b[38;5;241m.\u001b[39mCLASSES \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mCLASSES\n\u001b[0;32m---> 94\u001b[0m \u001b[43mtrain_detector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_validate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - James Cook University/Desktop/COMP9444/project/HRFuser/mmdet/apis/train.py:124\u001b[0m, in \u001b[0;36mtrain_detector\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    117\u001b[0m     model \u001b[38;5;241m=\u001b[39m MMDistributedDataParallel(\n\u001b[1;32m    118\u001b[0m         model\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m    119\u001b[0m         device_ids\u001b[38;5;241m=\u001b[39m[torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()],\n\u001b[1;32m    120\u001b[0m         broadcast_buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m         find_unused_parameters\u001b[38;5;241m=\u001b[39mfind_unused_parameters)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     model \u001b[38;5;241m=\u001b[39m MMDataParallel(\n\u001b[0;32m--> 124\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, device_ids\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mgpu_ids)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# build runner\u001b[39;00m\n\u001b[1;32m    127\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m build_optimizer(model, cfg\u001b[38;5;241m.\u001b[39moptimizer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ef7c0",
   "metadata": {},
   "source": [
    "### References (To be cleaned up later)\n",
    "https://www.exposit.com/blog/computer-vision-object-detection-challenges-faced/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced2525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
